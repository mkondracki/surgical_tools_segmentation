{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "from hydra.utils import instantiate\n",
    "from hydra import compose, initialize_config_module \n",
    "from argparse import ArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/samba/320281459/code/sam2\n"
     ]
    }
   ],
   "source": [
    "%cd /home/samba/320281459/code/sam2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchdata tensordict-nightly\n",
    "!pip3 install tensorboard\n",
    "!pip3 install fvcore\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pythonpath_to_sys_path():\n",
    "    if \"PYTHONPATH\" not in os.environ or not os.environ[\"PYTHONPATH\"]:\n",
    "        return\n",
    "    sys.path = os.environ[\"PYTHONPATH\"].split(\":\") + sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "resolver 'get_method' is already registered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m use_cluster\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      7\u001b[0m args_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfigs/sam2.1_training/sam2.1_hiera_l_Cardio_finetune.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mregister_omegaconf_resolvers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/sam2/training/utils/train_utils.py:53\u001b[0m, in \u001b[0;36mregister_omegaconf_resolvers\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mregister_omegaconf_resolvers\u001b[39m():\n\u001b[0;32m---> 53\u001b[0m     \u001b[43mOmegaConf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_new_resolver\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget_method\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhydra\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_method\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     OmegaConf\u001b[38;5;241m.\u001b[39mregister_new_resolver(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_class\u001b[39m\u001b[38;5;124m\"\u001b[39m, hydra\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mget_class)\n\u001b[1;32m     55\u001b[0m     OmegaConf\u001b[38;5;241m.\u001b[39mregister_new_resolver(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m x, y: x \u001b[38;5;241m+\u001b[39m y)\n",
      "File \u001b[0;32m~/miniforge3/envs/sam/lib/python3.11/site-packages/omegaconf/omegaconf.py:403\u001b[0m, in \u001b[0;36mOmegaConf.register_new_resolver\u001b[0;34m(name, resolver, replace, use_cache)\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot use an empty resolver name\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m replace \u001b[38;5;129;01mand\u001b[39;00m OmegaConf\u001b[38;5;241m.\u001b[39mhas_resolver(name):\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresolver \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is already registered\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    406\u001b[0m     sig: Optional[inspect\u001b[38;5;241m.\u001b[39mSignature] \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(resolver)\n",
      "\u001b[0;31mValueError\u001b[0m: resolver 'get_method' is already registered"
     ]
    }
   ],
   "source": [
    "from training.utils.train_utils import makedir, register_omegaconf_resolvers\n",
    "\n",
    "num_gpus=1\n",
    "num_nodes=1\n",
    "use_cluster=None\n",
    "\n",
    "args_config = 'configs/sam2.1_training/sam2.1_hiera_l_Cardio_finetune.yaml'\n",
    "\n",
    "register_omegaconf_resolvers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose\n",
    "from omegaconf import OmegaConf\n",
    "from iopath.common.file_io import g_pathmgr\n",
    "\n",
    "cfg = compose(config_name=args_config)\n",
    "# print(OmegaConf.to_yaml(cfg))\n",
    "# instantiate(cfg.trainer, _recursive_=False)\n",
    "if cfg.launcher.experiment_log_dir is None:\n",
    "    cfg.launcher.experiment_log_dir = os.path.join(\n",
    "        os.getcwd(), \"sam2_logs\", args_config\n",
    "    )\n",
    "\n",
    "add_pythonpath_to_sys_path()\n",
    "makedir(cfg.launcher.experiment_log_dir)\n",
    "with g_pathmgr.open(\n",
    "    os.path.join(cfg.launcher.experiment_log_dir, \"config.yaml\"), \"w\"\n",
    ") as f:\n",
    "    f.write(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "cfg_resolved = OmegaConf.to_container(cfg, resolve=False)\n",
    "cfg_resolved = OmegaConf.create(cfg_resolved)\n",
    "\n",
    "with g_pathmgr.open(\n",
    "    os.path.join(cfg.launcher.experiment_log_dir, \"config_resolved.yaml\"), \"w\"\n",
    ") as f:\n",
    "    f.write(OmegaConf.to_yaml(cfg_resolved, resolve=True))\n",
    "\n",
    "submitit_conf = cfg.get(\"submitit\", None)\n",
    "assert submitit_conf is not None, \"Missing submitit config\"\n",
    "\n",
    "submitit_dir = cfg.launcher.experiment_log_dir\n",
    "submitit_dir = os.path.join(submitit_dir, \"submitit_logs\")\n",
    "# Priotrize cmd line args\n",
    "cfg.launcher.gpus_per_node = (\n",
    "    num_gpus if num_gpus is not None else cfg.launcher.gpus_per_node\n",
    ")\n",
    "cfg.launcher.num_nodes = (\n",
    "    num_nodes if num_nodes is not None else cfg.launcher.num_nodes\n",
    ")\n",
    "submitit_conf.use_cluster = (\n",
    "    use_cluster if use_cluster is not None else submitit_conf.use_cluster\n",
    ")\n",
    "\n",
    "cfg.launcher.num_nodes = 1\n",
    "main_port = random.randint(\n",
    "    submitit_conf.port_range[0], submitit_conf.port_range[1]\n",
    ")\n",
    "\n",
    "num_proc = cfg.launcher.gpus_per_node\n",
    "torch.multiprocessing.set_start_method(\n",
    "    \"spawn\"\n",
    ")  # CUDA runtime does not support `fork`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dataset length = 1342\n",
      "INFO 2025-03-26 12:17:31,422 sam2_datasets.py: 125: Dataset mixing probabilities: [1.0]\n"
     ]
    }
   ],
   "source": [
    "val_dataset = instantiate(cfg.trainer.data.get('val'))\n",
    "dataloader = val_dataset.get_loader(epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchedVideoDatapoint(\n",
      "    dict_key=NonTensorData(data=val, batch_size=torch.Size([1]), device=None),\n",
      "    img_batch=Tensor(shape=torch.Size([1, 20, 3, 512, 512]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "    masks=Tensor(shape=torch.Size([1, 50, 512, 512]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "    metadata=BatchedVideoMetaData(\n",
      "        frame_orig_size=Tensor(shape=torch.Size([1, 50, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        unique_objects_identifier=Tensor(shape=torch.Size([1, 50, 3]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        batch_size=torch.Size([1]),\n",
      "        device=None,\n",
      "        is_shared=False),\n",
      "    obj_to_frame_idx=Tensor(shape=torch.Size([1, 50, 2]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "    batch_size=torch.Size([1]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "for i in dataloader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-03-26 11:47:07,153 train_utils.py: 108: MACHINE SEED: 4920\n",
      "INFO 2025-03-26 11:47:07,157 train_utils.py: 154: Logging ENV_VARIABLES\n",
      "INFO 2025-03-26 11:47:07,157 train_utils.py: 155: BROWSER=/home/samba/320281459/.vscode-server/cli/servers/Stable-ddc367ed5c8936efe395cffeec279b04ffd7db78/server/bin/helpers/browser.sh\n",
      "CLICOLOR=1\n",
      "CLICOLOR_FORCE=1\n",
      "CONDA_DEFAULT_ENV=sam\n",
      "CONDA_EXE=/home/samba/320281459/miniforge3/bin/conda\n",
      "CONDA_PREFIX=/home/samba/320281459/miniforge3/envs/sam\n",
      "CONDA_PROMPT_MODIFIER=(sam) \n",
      "CONDA_PYTHON_EXE=/home/samba/320281459/miniforge3/bin/python\n",
      "CONDA_ROOT=/home/samba/320281459/miniforge3\n",
      "CONDA_SHLVL=1\n",
      "CUDA_MODULE_LOADING=LAZY\n",
      "DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/31400531/bus\n",
      "ELECTRON_RUN_AS_NODE=1\n",
      "FORCE_COLOR=1\n",
      "FTP_PROXY=http://146.112.255.50:80/\n",
      "GIT_PAGER=cat\n",
      "HOME=/home/samba/320281459\n",
      "HTTPS_PROXY=http://146.112.255.50:443/\n",
      "HTTP_PROXY=http://146.112.255.50:80/\n",
      "LANG=en_US.UTF-8\n",
      "LD_LIBRARY_PATH=/home/samba/320281459/miniforge3/envs/sam/lib/python3.11/site-packages/cv2/../../lib64:\n",
      "LOCAL_RANK=0\n",
      "LOGNAME=CODE1\\320281459\n",
      "MASTER_ADDR=localhost\n",
      "MASTER_PORT=25103\n",
      "MOTD_SHOWN=pam\n",
      "MPLBACKEND=module://matplotlib_inline.backend_inline\n",
      "NO_PROXY=localhost,127.0.0.1,frqsurmed1lx001,.code1.emi.philips.com,.philips.com,.fr-101.lan.philips.com,130.143.0.0/16,130.139.0.0/16\n",
      "PAGER=cat\n",
      "PATH=/home/samba/320281459/miniforge3/envs/sam/bin:/home/samba/320281459/.vscode-server/cli/servers/Stable-ddc367ed5c8936efe395cffeec279b04ffd7db78/server/bin/remote-cli:/home/samba/320281459/miniforge3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n",
      "PWD=/home/samba/320281459\n",
      "PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING=1\n",
      "PYDEVD_USE_FRAME_EVAL=NO\n",
      "PYTHONIOENCODING=utf-8\n",
      "PYTHONUNBUFFERED=1\n",
      "PYTHON_FROZEN_MODULES=on\n",
      "QT_QPA_FONTDIR=/home/samba/320281459/miniforge3/envs/sam/lib/python3.11/site-packages/cv2/qt/fonts\n",
      "QT_QPA_PLATFORM_PLUGIN_PATH=/home/samba/320281459/miniforge3/envs/sam/lib/python3.11/site-packages/cv2/qt/plugins\n",
      "RANK=0\n",
      "REQUESTS_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt\n",
      "SHELL=/bin/bash\n",
      "SHLVL=1\n",
      "SSH_CLIENT=130.139.177.219 54555 22\n",
      "SSH_CONNECTION=130.139.177.219 54555 130.139.179.40 22\n",
      "SSL_CERT_DIR=/usr/lib/ssl/certs\n",
      "SSL_CERT_FILE=/usr/lib/ssl/certs/ca-certificates.crt\n",
      "TERM=xterm-color\n",
      "TORCH_NCCL_ASYNC_ERROR_HANDLING=1\n",
      "USER=CODE1\\320281459\n",
      "VSCODE_AGENT_FOLDER=/home/samba/320281459/.vscode-server\n",
      "VSCODE_CLI_REQUIRE_TOKEN=3ccf02fe-c306-4176-b2cd-0b7dd9cdfbd5\n",
      "VSCODE_CWD=/home/samba/320281459\n",
      "VSCODE_ESM_ENTRYPOINT=vs/workbench/api/node/extensionHostProcess\n",
      "VSCODE_HANDLES_SIGPIPE=true\n",
      "VSCODE_HANDLES_UNCAUGHT_ERRORS=true\n",
      "VSCODE_IPC_HOOK_CLI=/run/user/31400531/vscode-ipc-0ed93c34-be80-466b-afb9-3b1a25d486d1.sock\n",
      "VSCODE_NLS_CONFIG={\"userLocale\":\"en\",\"osLocale\":\"en\",\"resolvedLanguage\":\"en\",\"defaultMessagesFile\":\"/home/samba/320281459/.vscode-server/cli/servers/Stable-ddc367ed5c8936efe395cffeec279b04ffd7db78/server/out/nls.messages.json\",\"locale\":\"en\",\"availableLanguages\":{}}\n",
      "WORLD_SIZE=1\n",
      "XDG_DATA_DIRS=/usr/local/share:/usr/share:/var/lib/snapd/desktop\n",
      "XDG_RUNTIME_DIR=/run/user/31400531\n",
      "XDG_SESSION_CLASS=user\n",
      "XDG_SESSION_ID=14368\n",
      "XDG_SESSION_TYPE=tty\n",
      "_=/home/samba/320281459/miniforge3/envs/sam/bin/python\n",
      "_CE_CONDA=\n",
      "_CE_M=\n",
      "ftp_proxy=http://146.112.255.50:80/\n",
      "http_proxy=http://146.112.255.50:80/\n",
      "https_proxy=http://146.112.255.50:443/\n",
      "no_proxy=localhost,127.0.0.1,frqsurmed1lx001,.code1.emi.philips.com,.philips.com,.fr-101.lan.philips.com,130.143.0.0/16,130.139.0.0/16\n",
      "\n",
      "INFO 2025-03-26 11:47:07,159 trainer.py: 989: Setting up components: Model, loss, optim, meters etc.\n",
      "INFO 2025-03-26 11:47:07,161 logger.py:  66: TensorBoard SummaryWriter instantiated. Files will be stored in: /home/samba/320281459/code/sam2/sam2_logs/configs/sam2.1_training/sam2.1_hiera_l_Cardio_finetune.yaml/tensorboard\n",
      "INFO 2025-03-26 11:47:08,616 sam2.py:  82: Training with points (sampled from masks) as inputs with p=0.5\n",
      "INFO 2025-03-26 11:47:08,621 trainer.py:1059: ====================\n",
      "INFO 2025-03-26 11:47:08,621 trainer.py:1060: Summary for model <class 'training.model.sam2.SAM2Train'>\n",
      "INFO 2025-03-26 11:47:08,624 trainer.py:1061: Model is SAM2Train(\n",
      "  (image_encoder): ImageEncoder(\n",
      "    (trunk): Hiera(\n",
      "      (patch_embed): PatchEmbed(\n",
      "        (proj): Conv2d(3, 144, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
      "      )\n",
      "      (blocks): ModuleList(\n",
      "        (0): MultiScaleBlock(\n",
      "          (norm1): LayerNorm((144,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): LoraMultiScaleAttention(\n",
      "            (qkv): Linear(in_features=144, out_features=432, bias=True)\n",
      "            (proj): Linear(in_features=144, out_features=144, bias=True)\n",
      "            (lora_q): Lora(\n",
      "              (linear1): Linear(in_features=144, out_features=4, bias=False)\n",
      "              (linear2): Linear(in_features=4, out_features=144, bias=False)\n",
      "            )\n",
      "            (lora_v): Lora(\n",
      "              (linear1): Linear(in_features=144, out_features=4, bias=False)\n",
      "              (linear2): Linear(in_features=4, out_features=144, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((144,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=144, out_features=576, bias=True)\n",
      "              (1): Linear(in_features=576, out_features=144, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "        (1): MultiScaleBlock(\n",
      "          (norm1): LayerNorm((144,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): LoraMultiScaleAttention(\n",
      "            (qkv): Linear(in_features=144, out_features=432, bias=True)\n",
      "            (proj): Linear(in_features=144, out_features=144, bias=True)\n",
      "            (lora_q): Lora(\n",
      "              (linear1): Linear(in_features=144, out_features=4, bias=False)\n",
      "              (linear2): Linear(in_features=4, out_features=144, bias=False)\n",
      "            )\n",
      "            (lora_v): Lora(\n",
      "              (linear1): Linear(in_features=144, out_features=4, bias=False)\n",
      "              (linear2): Linear(in_features=4, out_features=144, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((144,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=144, out_features=576, bias=True)\n",
      "              (1): Linear(in_features=576, out_features=144, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "        (2): MultiScaleBlock(\n",
      "          (norm1): LayerNorm((144,), eps=1e-06, elementwise_affine=True)\n",
      "          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "          (attn): LoraMultiScaleAttention(\n",
      "            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "            (qkv): Linear(in_features=144, out_features=864, bias=True)\n",
      "            (proj): Linear(in_features=288, out_features=288, bias=True)\n",
      "            (lora_q): Lora(\n",
      "              (linear1): Linear(in_features=288, out_features=4, bias=False)\n",
      "              (linear2): Linear(in_features=4, out_features=288, bias=False)\n",
      "            )\n",
      "            (lora_v): Lora(\n",
      "              (linear1): Linear(in_features=288, out_features=4, bias=False)\n",
      "              (linear2): Linear(in_features=4, out_features=288, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((288,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=288, out_features=1152, bias=True)\n",
      "              (1): Linear(in_features=1152, out_features=288, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "          (proj): Linear(in_features=144, out_features=288, bias=True)\n",
      "        )\n",
      "        (3-7): 5 x MultiScaleBlock(\n",
      "          (norm1): LayerNorm((288,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): LoraMultiScaleAttention(\n",
      "            (qkv): Linear(in_features=288, out_features=864, bias=True)\n",
      "            (proj): Linear(in_features=288, out_features=288, bias=True)\n",
      "            (lora_q): Lora(\n",
      "              (linear1): Linear(in_features=288, out_features=4, bias=False)\n",
      "              (linear2): Linear(in_features=4, out_features=288, bias=False)\n",
      "            )\n",
      "            (lora_v): Lora(\n",
      "              (linear1): Linear(in_features=288, out_features=4, bias=False)\n",
      "              (linear2): Linear(in_features=4, out_features=288, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((288,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=288, out_features=1152, bias=True)\n",
      "              (1): Linear(in_features=1152, out_features=288, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "        (8): MultiScaleBlock(\n",
      "          (norm1): LayerNorm((288,), eps=1e-06, elementwise_affine=True)\n",
      "          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "          (attn): LoraMultiScaleAttention(\n",
      "            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "            (qkv): Linear(in_features=288, out_features=1728, bias=True)\n",
      "            (proj): Linear(in_features=576, out_features=576, bias=True)\n",
      "            (lora_q): Lora(\n",
      "              (linear1): Linear(in_features=576, out_features=4, bias=False)\n",
      "              (linear2): Linear(in_features=4, out_features=576, bias=False)\n",
      "            )\n",
      "            (lora_v): Lora(\n",
      "              (linear1): Linear(in_features=576, out_features=4, bias=False)\n",
      "              (linear2): Linear(in_features=4, out_features=576, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=576, out_features=2304, bias=True)\n",
      "              (1): Linear(in_features=2304, out_features=576, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "          (proj): Linear(in_features=288, out_features=576, bias=True)\n",
      "        )\n",
      "        (9-43): 35 x MultiScaleBlock(\n",
      "          (norm1): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): LoraMultiScaleAttention(\n",
      "            (qkv): Linear(in_features=576, out_features=1728, bias=True)\n",
      "            (proj): Linear(in_features=576, out_features=576, bias=True)\n",
      "            (lora_q): Lora(\n",
      "              (linear1): Linear(in_features=576, out_features=4, bias=False)\n",
      "              (linear2): Linear(in_features=4, out_features=576, bias=False)\n",
      "            )\n",
      "            (lora_v): Lora(\n",
      "              (linear1): Linear(in_features=576, out_features=4, bias=False)\n",
      "              (linear2): Linear(in_features=4, out_features=576, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=576, out_features=2304, bias=True)\n",
      "              (1): Linear(in_features=2304, out_features=576, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "        (44): MultiScaleBlock(\n",
      "          (norm1): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
      "          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "          (attn): LoraMultiScaleAttention(\n",
      "            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "            (qkv): Linear(in_features=576, out_features=3456, bias=True)\n",
      "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "            (lora_q): Lora(\n",
      "              (linear1): Linear(in_features=1152, out_features=4, bias=False)\n",
      "              (linear2): Linear(in_features=4, out_features=1152, bias=False)\n",
      "            )\n",
      "            (lora_v): Lora(\n",
      "              (linear1): Linear(in_features=1152, out_features=4, bias=False)\n",
      "              (linear2): Linear(in_features=4, out_features=1152, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=1152, out_features=4608, bias=True)\n",
      "              (1): Linear(in_features=4608, out_features=1152, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "          (proj): Linear(in_features=576, out_features=1152, bias=True)\n",
      "        )\n",
      "        (45-47): 3 x MultiScaleBlock(\n",
      "          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): LoraMultiScaleAttention(\n",
      "            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
      "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "            (lora_q): Lora(\n",
      "              (linear1): Linear(in_features=1152, out_features=4, bias=False)\n",
      "              (linear2): Linear(in_features=4, out_features=1152, bias=False)\n",
      "            )\n",
      "            (lora_v): Lora(\n",
      "              (linear1): Linear(in_features=1152, out_features=4, bias=False)\n",
      "              (linear2): Linear(in_features=4, out_features=1152, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=1152, out_features=4608, bias=True)\n",
      "              (1): Linear(in_features=4608, out_features=1152, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (neck): FpnNeck(\n",
      "      (position_encoding): PositionEmbeddingSine()\n",
      "      (convs): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (conv): Conv2d(1152, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (conv): Conv2d(576, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (conv): Conv2d(288, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (conv): Conv2d(144, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mask_downsample): Conv2d(1, 1, kernel_size=(4, 4), stride=(4, 4))\n",
      "  (memory_attention): MemoryAttention(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x MemoryAttentionLayer(\n",
      "        (self_attn): RoPEAttention(\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (cross_attn_image): RoPEAttention(\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (k_proj): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (memory_encoder): MemoryEncoder(\n",
      "    (mask_downsampler): MaskDownSampler(\n",
      "      (encoder): Sequential(\n",
      "        (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): LayerNorm2d()\n",
      "        (2): GELU(approximate='none')\n",
      "        (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (4): LayerNorm2d()\n",
      "        (5): GELU(approximate='none')\n",
      "        (6): Conv2d(16, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (7): LayerNorm2d()\n",
      "        (8): GELU(approximate='none')\n",
      "        (9): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (10): LayerNorm2d()\n",
      "        (11): GELU(approximate='none')\n",
      "        (12): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (pix_feat_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fuser): Fuser(\n",
      "      (proj): Identity()\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x CXBlock(\n",
      "          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
      "          (norm): LayerNorm2d()\n",
      "          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (position_encoding): PositionEmbeddingSine()\n",
      "    (out_proj): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (sam_prompt_encoder): PromptEncoder(\n",
      "    (pe_layer): PositionEmbeddingRandom()\n",
      "    (point_embeddings): ModuleList(\n",
      "      (0-3): 4 x Embedding(1, 256)\n",
      "    )\n",
      "    (not_a_point_embed): Embedding(1, 256)\n",
      "    (mask_downscaling): Sequential(\n",
      "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (1): LayerNorm2d()\n",
      "      (2): GELU(approximate='none')\n",
      "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (4): LayerNorm2d()\n",
      "      (5): GELU(approximate='none')\n",
      "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (no_mask_embed): Embedding(1, 256)\n",
      "  )\n",
      "  (sam_mask_decoder): MaskDecoder(\n",
      "    (transformer): TwoWayTransformer(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x TwoWayAttentionBlock(\n",
      "          (self_attn): Attention(\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (cross_attn_token_to_image): Attention(\n",
      "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "              (1): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            )\n",
      "            (act): ReLU()\n",
      "          )\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (cross_attn_image_to_token): Attention(\n",
      "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_attn_token_to_image): Attention(\n",
      "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
      "      )\n",
      "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (iou_token): Embedding(1, 256)\n",
      "    (mask_tokens): Embedding(4, 256)\n",
      "    (obj_score_token): Embedding(1, 256)\n",
      "    (output_upscaling): Sequential(\n",
      "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (1): LayerNorm2d()\n",
      "      (2): GELU(approximate='none')\n",
      "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (4): GELU(approximate='none')\n",
      "    )\n",
      "    (conv_s0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv_s1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (output_hypernetworks_mlps): ModuleList(\n",
      "      (0-3): 4 x MLP(\n",
      "        (layers): ModuleList(\n",
      "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
      "        )\n",
      "        (act): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (iou_prediction_head): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "      (act): ReLU()\n",
      "    )\n",
      "    (pred_obj_score_head): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "      )\n",
      "      (act): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (obj_ptr_proj): MLP(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (act): ReLU()\n",
      "  )\n",
      "  (obj_ptr_tpos_proj): Linear(in_features=256, out_features=64, bias=True)\n",
      ")\n",
      "INFO 2025-03-26 11:47:08,624 trainer.py:1062: \tTotal parameters 224 M\n",
      "INFO 2025-03-26 11:47:08,624 trainer.py:1063: \tTrainable parameters 12.2 M\n",
      "INFO 2025-03-26 11:47:08,625 trainer.py:1066: \tNon-Trainable parameters 212 M\n",
      "INFO 2025-03-26 11:47:08,625 trainer.py:1069: ====================\n",
      "INFO 2025-03-26 11:47:08,627 trainer.py:1023: Finished setting up components: Model, loss, optim, meters etc.\n",
      "INFO 2025-03-26 11:47:08,628 trainer.py: 314: Moving components to device cuda:0 and local rank 0.\n",
      "INFO 2025-03-26 11:47:08,804 trainer.py: 320: Done moving components to device cuda:0 and local rank 0.\n",
      "INFO 2025-03-26 11:47:08,816 optimizer.py: 248: Matches for param_name [image_encoder.*]: {'image_encoder.trunk.blocks.12.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.16.mlp.layers.0.bias', 'image_encoder.trunk.blocks.19.mlp.layers.1.bias', 'image_encoder.trunk.blocks.20.norm2.weight', 'image_encoder.trunk.blocks.37.norm1.bias', 'image_encoder.trunk.blocks.0.mlp.layers.1.weight', 'image_encoder.trunk.blocks.17.norm1.weight', 'image_encoder.trunk.blocks.42.mlp.layers.1.bias', 'image_encoder.trunk.blocks.38.mlp.layers.0.bias', 'image_encoder.trunk.blocks.22.attn.proj.weight', 'image_encoder.trunk.blocks.1.attn.qkv.weight', 'image_encoder.trunk.blocks.31.mlp.layers.1.bias', 'image_encoder.trunk.blocks.33.attn.qkv.weight', 'image_encoder.trunk.blocks.47.norm2.weight', 'image_encoder.trunk.blocks.27.mlp.layers.0.bias', 'image_encoder.trunk.blocks.39.mlp.layers.1.weight', 'image_encoder.trunk.blocks.17.attn.proj.bias', 'image_encoder.neck.convs.1.conv.weight', 'image_encoder.trunk.blocks.8.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.43.mlp.layers.0.weight', 'image_encoder.trunk.blocks.4.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.33.attn.proj.weight', 'image_encoder.trunk.blocks.11.mlp.layers.0.bias', 'image_encoder.trunk.blocks.42.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.30.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.17.attn.qkv.weight', 'image_encoder.trunk.blocks.40.norm2.bias', 'image_encoder.trunk.blocks.43.mlp.layers.1.weight', 'image_encoder.trunk.blocks.20.norm1.bias', 'image_encoder.trunk.blocks.14.attn.qkv.weight', 'image_encoder.trunk.blocks.6.mlp.layers.0.weight', 'image_encoder.trunk.blocks.45.attn.qkv.bias', 'image_encoder.trunk.blocks.35.attn.qkv.weight', 'image_encoder.trunk.blocks.34.mlp.layers.0.weight', 'image_encoder.trunk.blocks.11.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.39.attn.qkv.weight', 'image_encoder.trunk.blocks.32.mlp.layers.1.bias', 'image_encoder.trunk.blocks.24.attn.qkv.bias', 'image_encoder.trunk.blocks.18.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.16.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.43.norm1.weight', 'image_encoder.trunk.blocks.7.norm2.bias', 'image_encoder.trunk.blocks.26.norm2.weight', 'image_encoder.trunk.blocks.4.attn.proj.weight', 'image_encoder.trunk.blocks.17.norm1.bias', 'image_encoder.trunk.blocks.31.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.42.attn.proj.weight', 'image_encoder.trunk.blocks.27.mlp.layers.1.weight', 'image_encoder.trunk.blocks.30.attn.qkv.weight', 'image_encoder.trunk.blocks.36.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.32.attn.proj.bias', 'image_encoder.trunk.blocks.30.attn.proj.weight', 'image_encoder.trunk.blocks.46.norm2.weight', 'image_encoder.trunk.blocks.10.mlp.layers.1.weight', 'image_encoder.trunk.blocks.23.attn.qkv.bias', 'image_encoder.trunk.blocks.12.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.30.norm2.weight', 'image_encoder.trunk.blocks.14.attn.proj.bias', 'image_encoder.trunk.blocks.3.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.11.norm2.bias', 'image_encoder.trunk.blocks.22.attn.qkv.bias', 'image_encoder.trunk.blocks.35.attn.qkv.bias', 'image_encoder.trunk.blocks.22.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.24.mlp.layers.1.weight', 'image_encoder.trunk.blocks.10.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.26.mlp.layers.1.bias', 'image_encoder.trunk.blocks.9.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.12.attn.proj.weight', 'image_encoder.trunk.blocks.32.norm1.weight', 'image_encoder.trunk.blocks.28.attn.qkv.weight', 'image_encoder.trunk.blocks.21.attn.qkv.weight', 'image_encoder.trunk.blocks.19.norm2.bias', 'image_encoder.trunk.blocks.32.mlp.layers.0.bias', 'image_encoder.trunk.blocks.21.attn.proj.weight', 'image_encoder.trunk.blocks.20.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.22.norm2.weight', 'image_encoder.trunk.blocks.16.attn.qkv.weight', 'image_encoder.trunk.blocks.45.norm1.bias', 'image_encoder.trunk.blocks.1.norm2.bias', 'image_encoder.trunk.blocks.42.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.28.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.28.attn.lora_v.linear2.weight', 'image_encoder.trunk.pos_embed_window', 'image_encoder.trunk.blocks.39.norm2.bias', 'image_encoder.trunk.blocks.27.attn.proj.bias', 'image_encoder.trunk.blocks.37.norm1.weight', 'image_encoder.trunk.blocks.28.attn.qkv.bias', 'image_encoder.trunk.blocks.13.attn.proj.weight', 'image_encoder.trunk.blocks.15.mlp.layers.0.weight', 'image_encoder.trunk.blocks.26.mlp.layers.0.weight', 'image_encoder.trunk.blocks.47.norm1.bias', 'image_encoder.trunk.blocks.33.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.16.attn.qkv.bias', 'image_encoder.trunk.blocks.32.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.14.norm2.bias', 'image_encoder.trunk.blocks.37.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.23.attn.proj.bias', 'image_encoder.trunk.blocks.2.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.7.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.7.mlp.layers.0.weight', 'image_encoder.trunk.blocks.46.mlp.layers.1.weight', 'image_encoder.trunk.blocks.12.mlp.layers.0.weight', 'image_encoder.trunk.blocks.33.mlp.layers.1.weight', 'image_encoder.trunk.blocks.13.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.25.norm2.weight', 'image_encoder.trunk.blocks.44.norm1.bias', 'image_encoder.trunk.blocks.34.attn.qkv.bias', 'image_encoder.trunk.blocks.47.mlp.layers.1.bias', 'image_encoder.trunk.blocks.2.attn.qkv.weight', 'image_encoder.trunk.blocks.6.norm2.weight', 'image_encoder.trunk.blocks.41.norm2.weight', 'image_encoder.trunk.blocks.46.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.23.mlp.layers.0.bias', 'image_encoder.trunk.blocks.12.mlp.layers.0.bias', 'image_encoder.trunk.blocks.34.mlp.layers.0.bias', 'image_encoder.trunk.blocks.16.norm1.bias', 'image_encoder.trunk.blocks.21.mlp.layers.0.bias', 'image_encoder.trunk.blocks.26.norm1.bias', 'image_encoder.trunk.blocks.34.norm2.bias', 'image_encoder.trunk.blocks.30.mlp.layers.1.weight', 'image_encoder.trunk.blocks.3.mlp.layers.0.weight', 'image_encoder.trunk.blocks.10.mlp.layers.0.bias', 'image_encoder.trunk.blocks.20.mlp.layers.1.weight', 'image_encoder.trunk.blocks.8.norm2.bias', 'image_encoder.trunk.blocks.31.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.12.attn.qkv.bias', 'image_encoder.trunk.blocks.24.norm1.weight', 'image_encoder.trunk.blocks.41.norm2.bias', 'image_encoder.trunk.blocks.7.attn.qkv.bias', 'image_encoder.trunk.blocks.28.norm1.weight', 'image_encoder.trunk.blocks.12.norm1.bias', 'image_encoder.trunk.blocks.12.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.1.attn.proj.bias', 'image_encoder.trunk.blocks.5.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.13.attn.qkv.bias', 'image_encoder.trunk.blocks.30.attn.qkv.bias', 'image_encoder.trunk.blocks.5.attn.proj.bias', 'image_encoder.trunk.blocks.44.attn.proj.weight', 'image_encoder.trunk.blocks.12.norm2.bias', 'image_encoder.trunk.blocks.7.mlp.layers.1.bias', 'image_encoder.trunk.blocks.34.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.19.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.39.mlp.layers.0.bias', 'image_encoder.trunk.blocks.22.mlp.layers.0.bias', 'image_encoder.trunk.blocks.33.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.40.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.13.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.20.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.15.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.2.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.37.mlp.layers.0.bias', 'image_encoder.trunk.blocks.43.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.41.attn.qkv.bias', 'image_encoder.trunk.blocks.29.mlp.layers.0.bias', 'image_encoder.trunk.blocks.46.norm1.weight', 'image_encoder.trunk.blocks.31.norm1.weight', 'image_encoder.trunk.blocks.12.norm2.weight', 'image_encoder.trunk.blocks.24.mlp.layers.0.weight', 'image_encoder.trunk.blocks.43.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.35.norm2.bias', 'image_encoder.trunk.blocks.2.proj.weight', 'image_encoder.trunk.blocks.6.attn.qkv.weight', 'image_encoder.trunk.blocks.43.mlp.layers.0.bias', 'image_encoder.trunk.blocks.1.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.15.mlp.layers.1.weight', 'image_encoder.trunk.blocks.9.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.4.mlp.layers.0.bias', 'image_encoder.trunk.blocks.3.attn.qkv.weight', 'image_encoder.trunk.blocks.20.attn.proj.weight', 'image_encoder.trunk.blocks.0.attn.proj.weight', 'image_encoder.trunk.blocks.7.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.4.attn.qkv.bias', 'image_encoder.trunk.blocks.10.norm2.weight', 'image_encoder.trunk.blocks.29.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.38.attn.qkv.weight', 'image_encoder.trunk.blocks.8.proj.weight', 'image_encoder.trunk.blocks.45.norm2.bias', 'image_encoder.trunk.blocks.33.mlp.layers.0.bias', 'image_encoder.trunk.blocks.46.mlp.layers.0.bias', 'image_encoder.trunk.blocks.42.mlp.layers.0.weight', 'image_encoder.trunk.blocks.8.attn.proj.bias', 'image_encoder.trunk.blocks.7.norm1.weight', 'image_encoder.trunk.blocks.22.mlp.layers.1.bias', 'image_encoder.trunk.blocks.26.mlp.layers.0.bias', 'image_encoder.trunk.blocks.40.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.15.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.40.mlp.layers.0.weight', 'image_encoder.trunk.blocks.5.mlp.layers.0.bias', 'image_encoder.trunk.blocks.32.norm2.weight', 'image_encoder.neck.convs.0.conv.weight', 'image_encoder.trunk.blocks.41.mlp.layers.0.weight', 'image_encoder.trunk.blocks.43.norm2.weight', 'image_encoder.trunk.blocks.19.attn.qkv.weight', 'image_encoder.trunk.blocks.1.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.21.mlp.layers.1.bias', 'image_encoder.trunk.blocks.34.attn.qkv.weight', 'image_encoder.trunk.blocks.23.norm2.bias', 'image_encoder.trunk.blocks.38.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.13.norm1.bias', 'image_encoder.trunk.blocks.16.norm1.weight', 'image_encoder.trunk.blocks.14.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.6.norm1.bias', 'image_encoder.trunk.blocks.28.attn.proj.bias', 'image_encoder.trunk.blocks.4.mlp.layers.1.weight', 'image_encoder.trunk.blocks.25.mlp.layers.1.weight', 'image_encoder.trunk.blocks.30.norm1.bias', 'image_encoder.trunk.blocks.42.norm2.bias', 'image_encoder.trunk.blocks.45.attn.qkv.weight', 'image_encoder.trunk.blocks.20.attn.qkv.weight', 'image_encoder.trunk.blocks.16.norm2.bias', 'image_encoder.trunk.blocks.38.attn.proj.weight', 'image_encoder.trunk.blocks.7.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.35.attn.proj.bias', 'image_encoder.trunk.blocks.0.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.4.norm1.bias', 'image_encoder.trunk.blocks.8.mlp.layers.0.bias', 'image_encoder.trunk.blocks.41.attn.proj.bias', 'image_encoder.trunk.blocks.18.mlp.layers.0.weight', 'image_encoder.trunk.blocks.36.attn.qkv.bias', 'image_encoder.trunk.blocks.1.attn.proj.weight', 'image_encoder.trunk.blocks.44.mlp.layers.0.weight', 'image_encoder.trunk.blocks.10.norm2.bias', 'image_encoder.trunk.blocks.40.norm2.weight', 'image_encoder.trunk.blocks.46.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.5.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.25.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.27.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.11.attn.proj.bias', 'image_encoder.trunk.blocks.45.mlp.layers.1.weight', 'image_encoder.trunk.blocks.0.norm1.bias', 'image_encoder.trunk.blocks.45.mlp.layers.0.bias', 'image_encoder.trunk.blocks.5.attn.qkv.weight', 'image_encoder.trunk.blocks.24.attn.proj.weight', 'image_encoder.trunk.blocks.33.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.23.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.11.attn.qkv.bias', 'image_encoder.trunk.blocks.4.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.19.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.43.attn.proj.bias', 'image_encoder.trunk.blocks.40.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.32.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.23.mlp.layers.0.weight', 'image_encoder.trunk.blocks.0.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.11.mlp.layers.1.weight', 'image_encoder.trunk.blocks.21.attn.proj.bias', 'image_encoder.trunk.blocks.31.norm2.weight', 'image_encoder.trunk.blocks.40.attn.proj.bias', 'image_encoder.trunk.blocks.26.attn.proj.bias', 'image_encoder.trunk.blocks.10.attn.qkv.weight', 'image_encoder.trunk.blocks.39.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.2.norm2.bias', 'image_encoder.trunk.blocks.4.norm1.weight', 'image_encoder.trunk.blocks.44.mlp.layers.1.weight', 'image_encoder.trunk.blocks.16.attn.proj.weight', 'image_encoder.trunk.blocks.15.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.34.norm1.bias', 'image_encoder.trunk.blocks.42.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.26.attn.proj.weight', 'image_encoder.trunk.blocks.13.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.14.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.2.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.2.norm2.weight', 'image_encoder.trunk.blocks.32.mlp.layers.1.weight', 'image_encoder.trunk.blocks.0.norm1.weight', 'image_encoder.trunk.blocks.30.mlp.layers.1.bias', 'image_encoder.trunk.blocks.38.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.8.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.32.mlp.layers.0.weight', 'image_encoder.trunk.blocks.9.attn.qkv.weight', 'image_encoder.trunk.blocks.17.norm2.bias', 'image_encoder.trunk.blocks.36.norm1.bias', 'image_encoder.trunk.blocks.14.mlp.layers.0.bias', 'image_encoder.trunk.blocks.36.mlp.layers.1.bias', 'image_encoder.trunk.blocks.45.attn.proj.bias', 'image_encoder.trunk.blocks.41.mlp.layers.1.weight', 'image_encoder.trunk.blocks.43.mlp.layers.1.bias', 'image_encoder.trunk.blocks.32.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.21.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.30.attn.proj.bias', 'image_encoder.trunk.blocks.47.norm1.weight', 'image_encoder.trunk.blocks.38.attn.qkv.bias', 'image_encoder.trunk.blocks.34.attn.proj.weight', 'image_encoder.trunk.blocks.30.norm1.weight', 'image_encoder.trunk.blocks.21.mlp.layers.0.weight', 'image_encoder.trunk.blocks.42.attn.proj.bias', 'image_encoder.trunk.blocks.22.norm1.bias', 'image_encoder.trunk.blocks.2.proj.bias', 'image_encoder.trunk.blocks.16.mlp.layers.1.weight', 'image_encoder.trunk.blocks.35.mlp.layers.1.bias', 'image_encoder.trunk.blocks.41.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.33.mlp.layers.0.weight', 'image_encoder.trunk.blocks.42.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.19.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.40.attn.qkv.weight', 'image_encoder.trunk.blocks.18.mlp.layers.1.weight', 'image_encoder.trunk.blocks.44.attn.qkv.weight', 'image_encoder.trunk.blocks.19.norm1.weight', 'image_encoder.trunk.blocks.20.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.27.attn.proj.weight', 'image_encoder.trunk.blocks.39.attn.proj.weight', 'image_encoder.trunk.blocks.15.attn.proj.weight', 'image_encoder.trunk.blocks.23.attn.proj.weight', 'image_encoder.trunk.blocks.29.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.1.mlp.layers.1.bias', 'image_encoder.trunk.blocks.6.norm2.bias', 'image_encoder.trunk.blocks.11.attn.proj.weight', 'image_encoder.trunk.blocks.31.mlp.layers.0.bias', 'image_encoder.neck.convs.3.conv.bias', 'image_encoder.trunk.blocks.15.norm2.weight', 'image_encoder.trunk.blocks.23.norm1.bias', 'image_encoder.trunk.blocks.45.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.1.mlp.layers.0.bias', 'image_encoder.trunk.blocks.31.attn.proj.bias', 'image_encoder.trunk.blocks.39.mlp.layers.1.bias', 'image_encoder.trunk.blocks.28.mlp.layers.0.weight', 'image_encoder.trunk.blocks.2.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.28.mlp.layers.1.weight', 'image_encoder.trunk.blocks.45.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.8.mlp.layers.1.weight', 'image_encoder.trunk.blocks.10.attn.proj.bias', 'image_encoder.trunk.blocks.39.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.35.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.17.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.41.norm1.bias', 'image_encoder.trunk.blocks.1.mlp.layers.1.weight', 'image_encoder.trunk.blocks.44.norm2.weight', 'image_encoder.trunk.blocks.27.mlp.layers.0.weight', 'image_encoder.trunk.blocks.18.mlp.layers.0.bias', 'image_encoder.trunk.blocks.17.mlp.layers.1.bias', 'image_encoder.trunk.blocks.26.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.39.norm2.weight', 'image_encoder.trunk.blocks.35.mlp.layers.0.weight', 'image_encoder.trunk.blocks.42.attn.qkv.bias', 'image_encoder.trunk.blocks.14.mlp.layers.1.weight', 'image_encoder.trunk.blocks.41.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.3.attn.proj.bias', 'image_encoder.trunk.blocks.40.mlp.layers.1.bias', 'image_encoder.trunk.blocks.10.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.13.mlp.layers.0.weight', 'image_encoder.trunk.blocks.2.norm1.bias', 'image_encoder.trunk.blocks.38.norm1.weight', 'image_encoder.trunk.blocks.9.mlp.layers.1.weight', 'image_encoder.trunk.blocks.28.norm1.bias', 'image_encoder.trunk.blocks.0.mlp.layers.0.bias', 'image_encoder.trunk.blocks.23.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.30.mlp.layers.0.bias', 'image_encoder.trunk.blocks.37.norm2.weight', 'image_encoder.trunk.blocks.35.norm2.weight', 'image_encoder.trunk.blocks.42.norm1.bias', 'image_encoder.trunk.blocks.14.norm1.weight', 'image_encoder.trunk.blocks.34.norm1.weight', 'image_encoder.trunk.blocks.40.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.34.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.37.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.32.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.13.attn.proj.bias', 'image_encoder.trunk.blocks.9.mlp.layers.0.bias', 'image_encoder.trunk.blocks.23.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.0.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.24.norm2.bias', 'image_encoder.trunk.blocks.32.attn.qkv.bias', 'image_encoder.trunk.blocks.46.mlp.layers.1.bias', 'image_encoder.trunk.blocks.10.norm1.weight', 'image_encoder.trunk.blocks.0.norm2.bias', 'image_encoder.trunk.blocks.27.mlp.layers.1.bias', 'image_encoder.trunk.blocks.46.attn.qkv.weight', 'image_encoder.trunk.blocks.47.norm2.bias', 'image_encoder.trunk.blocks.22.attn.proj.bias', 'image_encoder.trunk.blocks.20.attn.qkv.bias', 'image_encoder.trunk.blocks.5.attn.lora_q.linear1.weight', 'image_encoder.neck.convs.2.conv.weight', 'image_encoder.trunk.blocks.20.norm1.weight', 'image_encoder.trunk.blocks.21.norm1.weight', 'image_encoder.trunk.blocks.6.mlp.layers.0.bias', 'image_encoder.trunk.blocks.30.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.5.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.37.attn.proj.weight', 'image_encoder.trunk.blocks.20.attn.proj.bias', 'image_encoder.trunk.blocks.43.norm2.bias', 'image_encoder.trunk.blocks.6.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.45.norm1.weight', 'image_encoder.trunk.blocks.19.norm1.bias', 'image_encoder.trunk.blocks.23.norm1.weight', 'image_encoder.trunk.blocks.25.mlp.layers.0.bias', 'image_encoder.trunk.blocks.23.mlp.layers.1.bias', 'image_encoder.trunk.blocks.44.norm2.bias', 'image_encoder.trunk.blocks.4.attn.qkv.weight', 'image_encoder.trunk.blocks.6.mlp.layers.1.weight', 'image_encoder.trunk.blocks.29.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.45.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.32.norm1.bias', 'image_encoder.trunk.blocks.37.mlp.layers.1.bias', 'image_encoder.trunk.blocks.41.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.37.mlp.layers.1.weight', 'image_encoder.trunk.blocks.22.attn.qkv.weight', 'image_encoder.trunk.blocks.15.attn.qkv.weight', 'image_encoder.trunk.blocks.19.mlp.layers.1.weight', 'image_encoder.trunk.blocks.25.norm1.bias', 'image_encoder.trunk.blocks.27.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.17.mlp.layers.0.bias', 'image_encoder.trunk.blocks.44.attn.qkv.bias', 'image_encoder.trunk.blocks.29.mlp.layers.1.weight', 'image_encoder.trunk.blocks.7.mlp.layers.1.weight', 'image_encoder.trunk.blocks.38.attn.proj.bias', 'image_encoder.trunk.blocks.12.attn.qkv.weight', 'image_encoder.trunk.blocks.44.proj.bias', 'image_encoder.trunk.blocks.29.attn.qkv.weight', 'image_encoder.trunk.blocks.31.mlp.layers.1.weight', 'image_encoder.trunk.blocks.25.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.38.norm2.bias', 'image_encoder.trunk.blocks.16.attn.proj.bias', 'image_encoder.trunk.blocks.14.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.39.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.41.norm1.weight', 'image_encoder.trunk.blocks.5.norm1.bias', 'image_encoder.trunk.blocks.18.norm2.bias', 'image_encoder.trunk.blocks.27.attn.qkv.bias', 'image_encoder.trunk.blocks.16.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.38.mlp.layers.0.weight', 'image_encoder.trunk.blocks.3.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.41.attn.qkv.weight', 'image_encoder.trunk.blocks.20.mlp.layers.0.weight', 'image_encoder.trunk.blocks.13.mlp.layers.0.bias', 'image_encoder.trunk.blocks.31.norm2.bias', 'image_encoder.trunk.blocks.35.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.18.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.33.attn.qkv.bias', 'image_encoder.trunk.blocks.39.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.16.norm2.weight', 'image_encoder.trunk.blocks.17.attn.proj.weight', 'image_encoder.trunk.blocks.34.mlp.layers.1.weight', 'image_encoder.trunk.blocks.21.norm1.bias', 'image_encoder.trunk.blocks.1.norm2.weight', 'image_encoder.trunk.blocks.25.norm1.weight', 'image_encoder.trunk.blocks.2.attn.qkv.bias', 'image_encoder.trunk.blocks.5.norm1.weight', 'image_encoder.trunk.blocks.25.attn.qkv.bias', 'image_encoder.trunk.blocks.5.attn.proj.weight', 'image_encoder.trunk.blocks.42.attn.qkv.weight', 'image_encoder.trunk.blocks.3.mlp.layers.1.bias', 'image_encoder.trunk.blocks.18.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.11.attn.qkv.weight', 'image_encoder.trunk.blocks.26.attn.qkv.weight', 'image_encoder.trunk.blocks.37.norm2.bias', 'image_encoder.trunk.blocks.9.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.42.norm1.weight', 'image_encoder.trunk.blocks.3.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.36.attn.proj.bias', 'image_encoder.trunk.blocks.3.attn.proj.weight', 'image_encoder.trunk.blocks.21.norm2.bias', 'image_encoder.trunk.blocks.31.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.14.attn.proj.weight', 'image_encoder.trunk.blocks.22.norm1.weight', 'image_encoder.trunk.blocks.17.norm2.weight', 'image_encoder.trunk.blocks.21.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.23.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.8.norm1.bias', 'image_encoder.trunk.blocks.24.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.24.attn.proj.bias', 'image_encoder.trunk.blocks.28.mlp.layers.1.bias', 'image_encoder.trunk.blocks.24.mlp.layers.1.bias', 'image_encoder.trunk.blocks.41.attn.proj.weight', 'image_encoder.trunk.blocks.44.mlp.layers.0.bias', 'image_encoder.trunk.blocks.5.mlp.layers.1.bias', 'image_encoder.trunk.blocks.14.attn.qkv.bias', 'image_encoder.trunk.blocks.7.attn.qkv.weight', 'image_encoder.trunk.blocks.8.mlp.layers.1.bias', 'image_encoder.trunk.blocks.26.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.10.norm1.bias', 'image_encoder.trunk.blocks.31.attn.qkv.weight', 'image_encoder.trunk.blocks.17.attn.qkv.bias', 'image_encoder.trunk.blocks.25.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.40.mlp.layers.1.weight', 'image_encoder.trunk.blocks.1.attn.qkv.bias', 'image_encoder.trunk.blocks.30.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.36.attn.proj.weight', 'image_encoder.trunk.blocks.17.mlp.layers.1.weight', 'image_encoder.trunk.blocks.3.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.0.attn.qkv.weight', 'image_encoder.trunk.blocks.6.mlp.layers.1.bias', 'image_encoder.trunk.blocks.6.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.8.proj.bias', 'image_encoder.trunk.blocks.12.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.19.attn.qkv.bias', 'image_encoder.trunk.blocks.33.norm2.weight', 'image_encoder.trunk.blocks.43.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.7.norm1.bias', 'image_encoder.trunk.blocks.28.attn.proj.weight', 'image_encoder.trunk.blocks.35.norm1.bias', 'image_encoder.trunk.blocks.29.norm2.weight', 'image_encoder.trunk.blocks.34.attn.proj.bias', 'image_encoder.trunk.blocks.3.mlp.layers.1.weight', 'image_encoder.trunk.blocks.15.attn.proj.bias', 'image_encoder.trunk.blocks.27.norm2.bias', 'image_encoder.trunk.blocks.41.mlp.layers.1.bias', 'image_encoder.trunk.blocks.0.attn.qkv.bias', 'image_encoder.trunk.blocks.17.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.29.attn.proj.weight', 'image_encoder.trunk.blocks.9.norm2.bias', 'image_encoder.trunk.blocks.3.norm2.weight', 'image_encoder.trunk.blocks.36.mlp.layers.1.weight', 'image_encoder.trunk.blocks.40.norm1.bias', 'image_encoder.trunk.blocks.44.mlp.layers.1.bias', 'image_encoder.trunk.blocks.3.norm2.bias', 'image_encoder.trunk.blocks.0.mlp.layers.1.bias', 'image_encoder.trunk.blocks.47.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.6.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.9.mlp.layers.0.weight', 'image_encoder.trunk.blocks.23.mlp.layers.1.weight', 'image_encoder.trunk.blocks.15.mlp.layers.0.bias', 'image_encoder.trunk.blocks.42.mlp.layers.0.bias', 'image_encoder.trunk.blocks.39.norm1.bias', 'image_encoder.trunk.blocks.41.attn.lora_q.linear2.weight', 'image_encoder.neck.convs.3.conv.weight', 'image_encoder.trunk.blocks.5.mlp.layers.1.weight', 'image_encoder.trunk.blocks.32.norm2.bias', 'image_encoder.neck.convs.2.conv.bias', 'image_encoder.trunk.blocks.40.attn.proj.weight', 'image_encoder.trunk.blocks.46.norm2.bias', 'image_encoder.trunk.blocks.9.attn.qkv.bias', 'image_encoder.trunk.blocks.5.attn.qkv.bias', 'image_encoder.trunk.blocks.46.attn.qkv.bias', 'image_encoder.trunk.blocks.37.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.18.attn.proj.weight', 'image_encoder.trunk.blocks.16.mlp.layers.0.weight', 'image_encoder.trunk.blocks.46.attn.proj.weight', 'image_encoder.trunk.patch_embed.proj.bias', 'image_encoder.trunk.blocks.34.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.21.mlp.layers.1.weight', 'image_encoder.trunk.blocks.8.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.20.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.45.attn.proj.weight', 'image_encoder.trunk.blocks.31.attn.proj.weight', 'image_encoder.trunk.blocks.11.norm1.weight', 'image_encoder.trunk.blocks.36.norm2.weight', 'image_encoder.trunk.blocks.4.mlp.layers.0.weight', 'image_encoder.trunk.blocks.28.mlp.layers.0.bias', 'image_encoder.trunk.blocks.25.attn.proj.bias', 'image_encoder.trunk.blocks.1.norm1.bias', 'image_encoder.trunk.blocks.9.norm1.weight', 'image_encoder.trunk.blocks.40.norm1.weight', 'image_encoder.trunk.blocks.46.mlp.layers.0.weight', 'image_encoder.trunk.blocks.44.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.17.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.25.mlp.layers.0.weight', 'image_encoder.trunk.blocks.11.mlp.layers.1.bias', 'image_encoder.trunk.blocks.46.attn.proj.bias', 'image_encoder.trunk.blocks.34.norm2.weight', 'image_encoder.trunk.blocks.29.mlp.layers.0.weight', 'image_encoder.trunk.blocks.6.norm1.weight', 'image_encoder.trunk.blocks.30.mlp.layers.0.weight', 'image_encoder.trunk.blocks.43.attn.qkv.bias', 'image_encoder.trunk.blocks.13.norm2.weight', 'image_encoder.trunk.blocks.18.attn.qkv.bias', 'image_encoder.trunk.blocks.10.attn.proj.weight', 'image_encoder.trunk.blocks.14.norm2.weight', 'image_encoder.trunk.blocks.26.mlp.layers.1.weight', 'image_encoder.trunk.blocks.24.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.47.mlp.layers.1.weight', 'image_encoder.trunk.blocks.29.mlp.layers.1.bias', 'image_encoder.trunk.blocks.46.norm1.bias', 'image_encoder.trunk.blocks.0.mlp.layers.0.weight', 'image_encoder.trunk.blocks.10.mlp.layers.0.weight', 'image_encoder.trunk.pos_embed', 'image_encoder.trunk.blocks.37.mlp.layers.0.weight', 'image_encoder.trunk.blocks.27.attn.qkv.weight', 'image_encoder.trunk.blocks.11.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.6.attn.proj.weight', 'image_encoder.trunk.blocks.20.mlp.layers.0.bias', 'image_encoder.trunk.blocks.43.attn.qkv.weight', 'image_encoder.trunk.blocks.35.mlp.layers.1.weight', 'image_encoder.trunk.blocks.39.attn.proj.bias', 'image_encoder.trunk.blocks.45.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.2.attn.proj.weight', 'image_encoder.trunk.blocks.4.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.35.mlp.layers.0.bias', 'image_encoder.trunk.blocks.18.norm1.weight', 'image_encoder.trunk.blocks.38.mlp.layers.1.bias', 'image_encoder.trunk.blocks.29.attn.qkv.bias', 'image_encoder.trunk.blocks.44.attn.proj.bias', 'image_encoder.trunk.blocks.2.mlp.layers.0.weight', 'image_encoder.trunk.blocks.29.norm1.bias', 'image_encoder.trunk.blocks.47.attn.qkv.bias', 'image_encoder.trunk.blocks.47.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.36.norm1.weight', 'image_encoder.trunk.blocks.1.mlp.layers.0.weight', 'image_encoder.trunk.blocks.27.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.38.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.15.attn.qkv.bias', 'image_encoder.trunk.blocks.15.mlp.layers.1.bias', 'image_encoder.trunk.blocks.27.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.22.norm2.bias', 'image_encoder.trunk.blocks.9.attn.proj.bias', 'image_encoder.trunk.blocks.0.attn.proj.bias', 'image_encoder.trunk.blocks.24.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.2.mlp.layers.1.weight', 'image_encoder.trunk.blocks.36.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.47.attn.proj.bias', 'image_encoder.trunk.blocks.2.mlp.layers.1.bias', 'image_encoder.trunk.blocks.21.norm2.weight', 'image_encoder.trunk.blocks.43.attn.proj.weight', 'image_encoder.trunk.blocks.4.mlp.layers.1.bias', 'image_encoder.trunk.blocks.2.mlp.layers.0.bias', 'image_encoder.trunk.blocks.12.attn.proj.bias', 'image_encoder.trunk.blocks.19.attn.proj.weight', 'image_encoder.trunk.blocks.26.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.22.mlp.layers.1.weight', 'image_encoder.trunk.blocks.38.norm2.weight', 'image_encoder.trunk.blocks.7.norm2.weight', 'image_encoder.trunk.blocks.2.norm1.weight', 'image_encoder.trunk.blocks.19.mlp.layers.0.bias', 'image_encoder.trunk.blocks.10.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.36.norm2.bias', 'image_encoder.trunk.blocks.31.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.13.norm1.weight', 'image_encoder.trunk.blocks.8.mlp.layers.0.weight', 'image_encoder.trunk.blocks.15.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.39.norm1.weight', 'image_encoder.trunk.blocks.45.norm2.weight', 'image_encoder.trunk.blocks.34.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.17.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.3.norm1.weight', 'image_encoder.trunk.blocks.11.norm1.bias', 'image_encoder.trunk.blocks.37.attn.qkv.bias', 'image_encoder.trunk.blocks.24.norm2.weight', 'image_encoder.trunk.blocks.24.mlp.layers.0.bias', 'image_encoder.trunk.blocks.31.norm1.bias', 'image_encoder.trunk.blocks.6.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.9.mlp.layers.1.bias', 'image_encoder.trunk.blocks.11.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.39.attn.qkv.bias', 'image_encoder.trunk.blocks.29.norm1.weight', 'image_encoder.trunk.blocks.10.mlp.layers.1.bias', 'image_encoder.trunk.blocks.23.attn.qkv.weight', 'image_encoder.trunk.blocks.35.attn.lora_q.linear1.weight', 'image_encoder.neck.convs.0.conv.bias', 'image_encoder.trunk.blocks.12.mlp.layers.1.bias', 'image_encoder.trunk.blocks.3.norm1.bias', 'image_encoder.trunk.blocks.20.mlp.layers.1.bias', 'image_encoder.trunk.blocks.14.mlp.layers.1.bias', 'image_encoder.trunk.blocks.18.norm1.bias', 'image_encoder.trunk.blocks.5.mlp.layers.0.weight', 'image_encoder.trunk.blocks.1.norm1.weight', 'image_encoder.trunk.blocks.44.norm1.weight', 'image_encoder.trunk.blocks.47.mlp.layers.0.bias', 'image_encoder.trunk.blocks.8.norm2.weight', 'image_encoder.trunk.blocks.26.attn.qkv.bias', 'image_encoder.trunk.blocks.40.attn.qkv.bias', 'image_encoder.trunk.blocks.25.norm2.bias', 'image_encoder.trunk.blocks.4.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.41.mlp.layers.0.bias', 'image_encoder.trunk.blocks.47.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.1.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.18.attn.lora_v.linear2.weight', 'image_encoder.trunk.patch_embed.proj.weight', 'image_encoder.trunk.blocks.33.norm1.bias', 'image_encoder.trunk.blocks.27.norm1.bias', 'image_encoder.trunk.blocks.13.attn.qkv.weight', 'image_encoder.trunk.blocks.7.attn.proj.weight', 'image_encoder.trunk.blocks.24.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.43.norm1.bias', 'image_encoder.trunk.blocks.45.mlp.layers.0.weight', 'image_encoder.trunk.blocks.18.mlp.layers.1.bias', 'image_encoder.trunk.blocks.33.attn.proj.bias', 'image_encoder.trunk.blocks.28.norm2.bias', 'image_encoder.trunk.blocks.38.mlp.layers.1.weight', 'image_encoder.trunk.blocks.6.attn.qkv.bias', 'image_encoder.trunk.blocks.22.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.34.mlp.layers.1.bias', 'image_encoder.trunk.blocks.45.mlp.layers.1.bias', 'image_encoder.trunk.blocks.7.mlp.layers.0.bias', 'image_encoder.trunk.blocks.8.attn.qkv.bias', 'image_encoder.trunk.blocks.9.norm1.bias', 'image_encoder.trunk.blocks.16.mlp.layers.1.bias', 'image_encoder.trunk.blocks.4.attn.proj.bias', 'image_encoder.trunk.blocks.25.attn.proj.weight', 'image_encoder.trunk.blocks.28.norm2.weight', 'image_encoder.trunk.blocks.37.attn.qkv.weight', 'image_encoder.trunk.blocks.44.proj.weight', 'image_encoder.trunk.blocks.10.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.12.mlp.layers.1.weight', 'image_encoder.trunk.blocks.27.norm2.weight', 'image_encoder.trunk.blocks.32.attn.qkv.weight', 'image_encoder.trunk.blocks.44.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.16.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.9.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.33.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.47.attn.qkv.weight', 'image_encoder.trunk.blocks.25.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.37.attn.proj.bias', 'image_encoder.trunk.blocks.38.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.35.norm1.weight', 'image_encoder.trunk.blocks.17.mlp.layers.0.weight', 'image_encoder.trunk.blocks.4.norm2.weight', 'image_encoder.trunk.blocks.8.norm1.weight', 'image_encoder.trunk.blocks.15.norm1.bias', 'image_encoder.neck.convs.1.conv.bias', 'image_encoder.trunk.blocks.13.mlp.layers.1.bias', 'image_encoder.trunk.blocks.5.norm2.bias', 'image_encoder.trunk.blocks.29.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.14.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.38.norm1.bias', 'image_encoder.trunk.blocks.3.mlp.layers.0.bias', 'image_encoder.trunk.blocks.18.norm2.weight', 'image_encoder.trunk.blocks.47.mlp.layers.0.weight', 'image_encoder.trunk.blocks.30.norm2.bias', 'image_encoder.trunk.blocks.0.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.8.attn.proj.weight', 'image_encoder.trunk.blocks.28.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.8.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.12.norm1.weight', 'image_encoder.trunk.blocks.46.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.0.norm2.weight', 'image_encoder.trunk.blocks.16.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.31.attn.qkv.bias', 'image_encoder.trunk.blocks.33.norm1.weight', 'image_encoder.trunk.blocks.11.norm2.weight', 'image_encoder.trunk.blocks.7.attn.proj.bias', 'image_encoder.trunk.blocks.22.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.24.attn.qkv.weight', 'image_encoder.trunk.blocks.13.norm2.bias', 'image_encoder.trunk.blocks.47.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.1.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.19.attn.proj.bias', 'image_encoder.trunk.blocks.19.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.10.attn.qkv.bias', 'image_encoder.trunk.blocks.32.attn.proj.weight', 'image_encoder.trunk.blocks.33.mlp.layers.1.bias', 'image_encoder.trunk.blocks.42.norm2.weight', 'image_encoder.trunk.blocks.43.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.36.mlp.layers.0.bias', 'image_encoder.trunk.blocks.31.mlp.layers.0.weight', 'image_encoder.trunk.blocks.13.mlp.layers.1.weight', 'image_encoder.trunk.blocks.33.norm2.bias', 'image_encoder.trunk.blocks.13.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.20.norm2.bias', 'image_encoder.trunk.blocks.7.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.42.mlp.layers.1.weight', 'image_encoder.trunk.blocks.44.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.21.attn.qkv.bias', 'image_encoder.trunk.blocks.6.attn.proj.bias', 'image_encoder.trunk.blocks.14.norm1.bias', 'image_encoder.trunk.blocks.9.attn.proj.weight', 'image_encoder.trunk.blocks.18.attn.qkv.weight', 'image_encoder.trunk.blocks.18.attn.proj.bias', 'image_encoder.trunk.blocks.30.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.28.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.46.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.15.norm1.weight', 'image_encoder.trunk.blocks.47.attn.proj.weight', 'image_encoder.trunk.blocks.27.norm1.weight', 'image_encoder.trunk.blocks.25.attn.qkv.weight', 'image_encoder.trunk.blocks.29.norm2.bias', 'image_encoder.trunk.blocks.22.mlp.layers.0.weight', 'image_encoder.trunk.blocks.36.attn.qkv.weight', 'image_encoder.trunk.blocks.39.mlp.layers.0.weight', 'image_encoder.trunk.blocks.19.norm2.weight', 'image_encoder.trunk.blocks.36.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.3.attn.qkv.bias', 'image_encoder.trunk.blocks.36.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.21.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.37.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.5.norm2.weight', 'image_encoder.trunk.blocks.26.norm1.weight', 'image_encoder.trunk.blocks.26.norm2.bias', 'image_encoder.trunk.blocks.44.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.24.norm1.bias', 'image_encoder.trunk.blocks.25.mlp.layers.1.bias', 'image_encoder.trunk.blocks.8.attn.qkv.weight', 'image_encoder.trunk.blocks.40.mlp.layers.0.bias', 'image_encoder.trunk.blocks.11.mlp.layers.0.weight', 'image_encoder.trunk.blocks.9.norm2.weight', 'image_encoder.trunk.blocks.36.mlp.layers.0.weight', 'image_encoder.trunk.blocks.4.norm2.bias', 'image_encoder.trunk.blocks.35.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.14.mlp.layers.0.weight', 'image_encoder.trunk.blocks.21.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.35.attn.proj.weight', 'image_encoder.trunk.blocks.29.attn.proj.bias', 'image_encoder.trunk.blocks.23.norm2.weight', 'image_encoder.trunk.blocks.26.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.11.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.2.attn.proj.bias', 'image_encoder.trunk.blocks.15.norm2.bias', 'image_encoder.trunk.blocks.19.mlp.layers.0.weight', 'image_encoder.trunk.blocks.22.attn.lora_q.linear1.weight'}\n",
      "INFO 2025-03-26 11:47:08,818 optimizer.py: 248: Matches for param_name [*bias*]: {'image_encoder.trunk.blocks.16.mlp.layers.0.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.2.bias', 'image_encoder.trunk.blocks.19.mlp.layers.1.bias', 'image_encoder.trunk.blocks.37.norm1.bias', 'image_encoder.trunk.blocks.42.mlp.layers.1.bias', 'image_encoder.trunk.blocks.38.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias', 'memory_attention.layers.0.self_attn.k_proj.bias', 'image_encoder.trunk.blocks.31.mlp.layers.1.bias', 'image_encoder.trunk.blocks.27.mlp.layers.0.bias', 'image_encoder.trunk.blocks.17.attn.proj.bias', 'memory_attention.layers.0.norm1.bias', 'image_encoder.trunk.blocks.11.mlp.layers.0.bias', 'image_encoder.trunk.blocks.40.norm2.bias', 'image_encoder.trunk.blocks.20.norm1.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.45.attn.qkv.bias', 'image_encoder.trunk.blocks.32.mlp.layers.1.bias', 'image_encoder.trunk.blocks.24.attn.qkv.bias', 'image_encoder.trunk.blocks.7.norm2.bias', 'image_encoder.trunk.blocks.17.norm1.bias', 'image_encoder.trunk.blocks.32.attn.proj.bias', 'sam_mask_decoder.transformer.layers.0.norm3.bias', 'image_encoder.trunk.blocks.23.attn.qkv.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.2.bias', 'image_encoder.trunk.blocks.14.attn.proj.bias', 'image_encoder.trunk.blocks.11.norm2.bias', 'image_encoder.trunk.blocks.22.attn.qkv.bias', 'image_encoder.trunk.blocks.35.attn.qkv.bias', 'image_encoder.trunk.blocks.26.mlp.layers.1.bias', 'image_encoder.trunk.blocks.19.norm2.bias', 'image_encoder.trunk.blocks.32.mlp.layers.0.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.0.bias', 'memory_attention.layers.1.cross_attn_image.v_proj.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias', 'image_encoder.trunk.blocks.45.norm1.bias', 'image_encoder.trunk.blocks.1.norm2.bias', 'memory_attention.layers.0.norm2.bias', 'memory_attention.layers.0.linear2.bias', 'image_encoder.trunk.blocks.39.norm2.bias', 'image_encoder.trunk.blocks.27.attn.proj.bias', 'sam_prompt_encoder.mask_downscaling.0.bias', 'sam_mask_decoder.transformer.layers.0.norm2.bias', 'image_encoder.trunk.blocks.28.attn.qkv.bias', 'image_encoder.trunk.blocks.47.norm1.bias', 'image_encoder.trunk.blocks.16.attn.qkv.bias', 'image_encoder.trunk.blocks.14.norm2.bias', 'image_encoder.trunk.blocks.23.attn.proj.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias', 'memory_attention.layers.1.norm2.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.2.bias', 'sam_prompt_encoder.mask_downscaling.6.bias', 'memory_encoder.fuser.layers.0.dwconv.bias', 'image_encoder.trunk.blocks.44.norm1.bias', 'image_encoder.trunk.blocks.34.attn.qkv.bias', 'image_encoder.trunk.blocks.47.mlp.layers.1.bias', 'memory_encoder.fuser.layers.1.norm.bias', 'image_encoder.trunk.blocks.23.mlp.layers.0.bias', 'memory_attention.layers.3.linear2.bias', 'image_encoder.trunk.blocks.12.mlp.layers.0.bias', 'image_encoder.trunk.blocks.34.mlp.layers.0.bias', 'image_encoder.trunk.blocks.16.norm1.bias', 'image_encoder.trunk.blocks.21.mlp.layers.0.bias', 'image_encoder.trunk.blocks.26.norm1.bias', 'image_encoder.trunk.blocks.34.norm2.bias', 'memory_attention.layers.2.self_attn.q_proj.bias', 'image_encoder.trunk.blocks.10.mlp.layers.0.bias', 'image_encoder.trunk.blocks.8.norm2.bias', 'image_encoder.trunk.blocks.12.attn.qkv.bias', 'image_encoder.trunk.blocks.41.norm2.bias', 'image_encoder.trunk.blocks.7.attn.qkv.bias', 'image_encoder.trunk.blocks.12.norm1.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias', 'image_encoder.trunk.blocks.1.attn.proj.bias', 'image_encoder.trunk.blocks.13.attn.qkv.bias', 'image_encoder.trunk.blocks.30.attn.qkv.bias', 'image_encoder.trunk.blocks.5.attn.proj.bias', 'memory_attention.layers.1.linear2.bias', 'image_encoder.trunk.blocks.12.norm2.bias', 'image_encoder.trunk.blocks.7.mlp.layers.1.bias', 'sam_mask_decoder.iou_prediction_head.layers.2.bias', 'image_encoder.trunk.blocks.39.mlp.layers.0.bias', 'image_encoder.trunk.blocks.22.mlp.layers.0.bias', 'image_encoder.trunk.blocks.37.mlp.layers.0.bias', 'image_encoder.trunk.blocks.41.attn.qkv.bias', 'image_encoder.trunk.blocks.29.mlp.layers.0.bias', 'image_encoder.trunk.blocks.35.norm2.bias', 'image_encoder.trunk.blocks.43.mlp.layers.0.bias', 'image_encoder.trunk.blocks.4.mlp.layers.0.bias', 'image_encoder.trunk.blocks.4.attn.qkv.bias', 'image_encoder.trunk.blocks.45.norm2.bias', 'image_encoder.trunk.blocks.46.mlp.layers.0.bias', 'image_encoder.trunk.blocks.33.mlp.layers.0.bias', 'image_encoder.trunk.blocks.8.attn.proj.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias', 'image_encoder.trunk.blocks.22.mlp.layers.1.bias', 'image_encoder.trunk.blocks.26.mlp.layers.0.bias', 'image_encoder.trunk.blocks.5.mlp.layers.0.bias', 'memory_attention.layers.3.norm3.bias', 'sam_mask_decoder.iou_prediction_head.layers.0.bias', 'memory_encoder.fuser.layers.0.pwconv1.bias', 'image_encoder.trunk.blocks.21.mlp.layers.1.bias', 'image_encoder.trunk.blocks.23.norm2.bias', 'image_encoder.trunk.blocks.13.norm1.bias', 'image_encoder.trunk.blocks.6.norm1.bias', 'sam_mask_decoder.transformer.layers.1.norm1.bias', 'image_encoder.trunk.blocks.28.attn.proj.bias', 'image_encoder.trunk.blocks.42.norm2.bias', 'image_encoder.trunk.blocks.30.norm1.bias', 'image_encoder.trunk.blocks.16.norm2.bias', 'sam_mask_decoder.transformer.layers.0.norm1.bias', 'sam_mask_decoder.conv_s0.bias', 'image_encoder.trunk.blocks.35.attn.proj.bias', 'image_encoder.trunk.blocks.4.norm1.bias', 'memory_attention.layers.3.norm2.bias', 'image_encoder.trunk.blocks.8.mlp.layers.0.bias', 'image_encoder.trunk.blocks.41.attn.proj.bias', 'image_encoder.trunk.blocks.36.attn.qkv.bias', 'image_encoder.trunk.blocks.10.norm2.bias', 'image_encoder.trunk.blocks.11.attn.proj.bias', 'image_encoder.trunk.blocks.0.norm1.bias', 'image_encoder.trunk.blocks.45.mlp.layers.0.bias', 'memory_attention.layers.0.self_attn.v_proj.bias', 'image_encoder.trunk.blocks.11.attn.qkv.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.1.bias', 'image_encoder.trunk.blocks.43.attn.proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.1.bias', 'image_encoder.trunk.blocks.21.attn.proj.bias', 'memory_attention.layers.0.cross_attn_image.q_proj.bias', 'image_encoder.trunk.blocks.40.attn.proj.bias', 'image_encoder.trunk.blocks.26.attn.proj.bias', 'sam_mask_decoder.transformer.layers.1.norm4.bias', 'image_encoder.trunk.blocks.2.norm2.bias', 'image_encoder.trunk.blocks.34.norm1.bias', 'memory_encoder.fuser.layers.0.pwconv2.bias', 'sam_mask_decoder.transformer.layers.0.norm4.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.out_proj.bias', 'memory_attention.layers.3.cross_attn_image.out_proj.bias', 'obj_ptr_proj.layers.2.bias', 'image_encoder.trunk.blocks.30.mlp.layers.1.bias', 'memory_encoder.fuser.layers.1.pwconv2.bias', 'mask_downsample.bias', 'sam_mask_decoder.conv_s1.bias', 'image_encoder.trunk.blocks.17.norm2.bias', 'image_encoder.trunk.blocks.36.norm1.bias', 'image_encoder.trunk.blocks.14.mlp.layers.0.bias', 'image_encoder.trunk.blocks.36.mlp.layers.1.bias', 'image_encoder.trunk.blocks.45.attn.proj.bias', 'memory_attention.layers.3.self_attn.v_proj.bias', 'image_encoder.trunk.blocks.43.mlp.layers.1.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias', 'image_encoder.trunk.blocks.30.attn.proj.bias', 'image_encoder.trunk.blocks.38.attn.qkv.bias', 'image_encoder.trunk.blocks.42.attn.proj.bias', 'image_encoder.trunk.blocks.22.norm1.bias', 'image_encoder.trunk.blocks.2.proj.bias', 'image_encoder.trunk.blocks.35.mlp.layers.1.bias', 'memory_attention.layers.2.norm2.bias', 'memory_attention.layers.0.self_attn.q_proj.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.q_proj.bias', 'sam_mask_decoder.output_upscaling.3.bias', 'memory_attention.layers.1.linear1.bias', 'image_encoder.trunk.blocks.1.mlp.layers.1.bias', 'image_encoder.trunk.blocks.6.norm2.bias', 'image_encoder.trunk.blocks.31.mlp.layers.0.bias', 'image_encoder.neck.convs.3.conv.bias', 'image_encoder.trunk.blocks.23.norm1.bias', 'memory_attention.layers.0.linear1.bias', 'memory_encoder.mask_downsampler.encoder.6.bias', 'image_encoder.trunk.blocks.1.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias', 'image_encoder.trunk.blocks.31.attn.proj.bias', 'image_encoder.trunk.blocks.39.mlp.layers.1.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias', 'image_encoder.trunk.blocks.10.attn.proj.bias', 'memory_attention.layers.3.linear1.bias', 'memory_encoder.mask_downsampler.encoder.12.bias', 'image_encoder.trunk.blocks.41.norm1.bias', 'memory_attention.layers.2.linear2.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.k_proj.bias', 'memory_attention.layers.1.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.18.mlp.layers.0.bias', 'image_encoder.trunk.blocks.17.mlp.layers.1.bias', 'image_encoder.trunk.blocks.42.attn.qkv.bias', 'image_encoder.trunk.blocks.3.attn.proj.bias', 'image_encoder.trunk.blocks.40.mlp.layers.1.bias', 'image_encoder.trunk.blocks.2.norm1.bias', 'image_encoder.trunk.blocks.28.norm1.bias', 'image_encoder.trunk.blocks.0.mlp.layers.0.bias', 'memory_attention.layers.0.norm3.bias', 'memory_attention.layers.1.norm1.bias', 'image_encoder.trunk.blocks.30.mlp.layers.0.bias', 'image_encoder.trunk.blocks.42.norm1.bias', 'memory_encoder.mask_downsampler.encoder.7.bias', 'image_encoder.trunk.blocks.13.attn.proj.bias', 'image_encoder.trunk.blocks.9.mlp.layers.0.bias', 'image_encoder.trunk.blocks.24.norm2.bias', 'image_encoder.trunk.blocks.32.attn.qkv.bias', 'image_encoder.trunk.blocks.46.mlp.layers.1.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.q_proj.bias', 'image_encoder.trunk.blocks.0.norm2.bias', 'image_encoder.trunk.blocks.27.mlp.layers.1.bias', 'image_encoder.trunk.blocks.47.norm2.bias', 'image_encoder.trunk.blocks.22.attn.proj.bias', 'image_encoder.trunk.blocks.20.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.out_proj.bias', 'image_encoder.trunk.blocks.6.mlp.layers.0.bias', 'image_encoder.trunk.blocks.20.attn.proj.bias', 'image_encoder.trunk.blocks.43.norm2.bias', 'image_encoder.trunk.blocks.19.norm1.bias', 'memory_attention.layers.2.cross_attn_image.k_proj.bias', 'sam_mask_decoder.transformer.layers.0.mlp.layers.1.bias', 'image_encoder.trunk.blocks.25.mlp.layers.0.bias', 'image_encoder.trunk.blocks.23.mlp.layers.1.bias', 'image_encoder.trunk.blocks.44.norm2.bias', 'image_encoder.trunk.blocks.32.norm1.bias', 'memory_attention.layers.2.norm3.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias', 'image_encoder.trunk.blocks.37.mlp.layers.1.bias', 'memory_attention.layers.2.cross_attn_image.v_proj.bias', 'image_encoder.trunk.blocks.25.norm1.bias', 'image_encoder.trunk.blocks.17.mlp.layers.0.bias', 'image_encoder.trunk.blocks.44.attn.qkv.bias', 'image_encoder.trunk.blocks.38.attn.proj.bias', 'image_encoder.trunk.blocks.44.proj.bias', 'image_encoder.trunk.blocks.38.norm2.bias', 'memory_attention.layers.2.self_attn.v_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.1.bias', 'image_encoder.trunk.blocks.16.attn.proj.bias', 'image_encoder.trunk.blocks.5.norm1.bias', 'image_encoder.trunk.blocks.18.norm2.bias', 'image_encoder.trunk.blocks.27.attn.qkv.bias', 'sam_mask_decoder.output_upscaling.1.bias', 'memory_attention.layers.1.self_attn.v_proj.bias', 'image_encoder.trunk.blocks.13.mlp.layers.0.bias', 'image_encoder.trunk.blocks.31.norm2.bias', 'sam_prompt_encoder.mask_downscaling.4.bias', 'image_encoder.trunk.blocks.33.attn.qkv.bias', 'image_encoder.trunk.blocks.21.norm1.bias', 'image_encoder.trunk.blocks.2.attn.qkv.bias', 'image_encoder.trunk.blocks.25.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.1.norm3.bias', 'image_encoder.trunk.blocks.3.mlp.layers.1.bias', 'image_encoder.trunk.blocks.37.norm2.bias', 'memory_encoder.mask_downsampler.encoder.3.bias', 'memory_attention.layers.2.cross_attn_image.q_proj.bias', 'image_encoder.trunk.blocks.36.attn.proj.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.v_proj.bias', 'image_encoder.trunk.blocks.21.norm2.bias', 'image_encoder.trunk.blocks.8.norm1.bias', 'image_encoder.trunk.blocks.24.attn.proj.bias', 'image_encoder.trunk.blocks.28.mlp.layers.1.bias', 'image_encoder.trunk.blocks.24.mlp.layers.1.bias', 'memory_encoder.mask_downsampler.encoder.1.bias', 'image_encoder.trunk.blocks.44.mlp.layers.0.bias', 'sam_mask_decoder.iou_prediction_head.layers.1.bias', 'image_encoder.trunk.blocks.5.mlp.layers.1.bias', 'image_encoder.trunk.blocks.14.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias', 'image_encoder.trunk.blocks.8.mlp.layers.1.bias', 'image_encoder.trunk.blocks.10.norm1.bias', 'memory_attention.layers.2.self_attn.out_proj.bias', 'memory_attention.layers.3.self_attn.q_proj.bias', 'image_encoder.trunk.blocks.17.attn.qkv.bias', 'image_encoder.trunk.blocks.1.attn.qkv.bias', 'image_encoder.trunk.blocks.6.mlp.layers.1.bias', 'image_encoder.trunk.blocks.8.proj.bias', 'image_encoder.trunk.blocks.19.attn.qkv.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.0.bias', 'image_encoder.trunk.blocks.7.norm1.bias', 'image_encoder.trunk.blocks.35.norm1.bias', 'image_encoder.trunk.blocks.34.attn.proj.bias', 'memory_attention.layers.1.cross_attn_image.q_proj.bias', 'image_encoder.trunk.blocks.15.attn.proj.bias', 'image_encoder.trunk.blocks.27.norm2.bias', 'image_encoder.trunk.blocks.41.mlp.layers.1.bias', 'image_encoder.trunk.blocks.0.attn.qkv.bias', 'image_encoder.trunk.blocks.9.norm2.bias', 'image_encoder.trunk.blocks.44.mlp.layers.1.bias', 'image_encoder.trunk.blocks.40.norm1.bias', 'image_encoder.trunk.blocks.3.norm2.bias', 'image_encoder.trunk.blocks.0.mlp.layers.1.bias', 'sam_mask_decoder.output_upscaling.0.bias', 'image_encoder.trunk.blocks.42.mlp.layers.0.bias', 'image_encoder.trunk.blocks.15.mlp.layers.0.bias', 'memory_attention.layers.2.norm1.bias', 'image_encoder.trunk.blocks.39.norm1.bias', 'memory_attention.layers.1.cross_attn_image.out_proj.bias', 'image_encoder.trunk.blocks.32.norm2.bias', 'image_encoder.neck.convs.2.conv.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias', 'image_encoder.trunk.blocks.46.norm2.bias', 'image_encoder.trunk.blocks.9.attn.qkv.bias', 'image_encoder.trunk.blocks.5.attn.qkv.bias', 'image_encoder.trunk.blocks.46.attn.qkv.bias', 'sam_prompt_encoder.mask_downscaling.3.bias', 'memory_encoder.fuser.layers.1.pwconv1.bias', 'sam_mask_decoder.pred_obj_score_head.layers.2.bias', 'image_encoder.trunk.patch_embed.proj.bias', 'memory_attention.layers.1.cross_attn_image.k_proj.bias', 'memory_encoder.fuser.layers.0.norm.bias', 'memory_attention.layers.2.cross_attn_image.out_proj.bias', 'image_encoder.trunk.blocks.28.mlp.layers.0.bias', 'image_encoder.trunk.blocks.25.attn.proj.bias', 'image_encoder.trunk.blocks.1.norm1.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.k_proj.bias', 'memory_attention.layers.3.cross_attn_image.q_proj.bias', 'sam_mask_decoder.pred_obj_score_head.layers.1.bias', 'image_encoder.trunk.blocks.11.mlp.layers.1.bias', 'image_encoder.trunk.blocks.46.attn.proj.bias', 'memory_attention.layers.0.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.43.attn.qkv.bias', 'image_encoder.trunk.blocks.18.attn.qkv.bias', 'image_encoder.trunk.blocks.29.mlp.layers.1.bias', 'image_encoder.trunk.blocks.46.norm1.bias', 'obj_ptr_tpos_proj.bias', 'memory_encoder.mask_downsampler.encoder.10.bias', 'image_encoder.trunk.blocks.20.mlp.layers.0.bias', 'image_encoder.trunk.blocks.39.attn.proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.0.bias', 'image_encoder.trunk.blocks.35.mlp.layers.0.bias', 'image_encoder.trunk.blocks.38.mlp.layers.1.bias', 'image_encoder.trunk.blocks.29.attn.qkv.bias', 'image_encoder.trunk.blocks.44.attn.proj.bias', 'sam_mask_decoder.pred_obj_score_head.layers.0.bias', 'image_encoder.trunk.blocks.29.norm1.bias', 'image_encoder.trunk.blocks.47.attn.qkv.bias', 'image_encoder.trunk.blocks.15.attn.qkv.bias', 'image_encoder.trunk.blocks.15.mlp.layers.1.bias', 'image_encoder.trunk.blocks.22.norm2.bias', 'memory_encoder.fuser.layers.1.dwconv.bias', 'image_encoder.trunk.blocks.9.attn.proj.bias', 'image_encoder.trunk.blocks.0.attn.proj.bias', 'image_encoder.trunk.blocks.47.attn.proj.bias', 'image_encoder.trunk.blocks.2.mlp.layers.1.bias', 'image_encoder.trunk.blocks.4.mlp.layers.1.bias', 'sam_mask_decoder.transformer.layers.1.norm2.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias', 'image_encoder.trunk.blocks.2.mlp.layers.0.bias', 'memory_attention.layers.3.self_attn.out_proj.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.k_proj.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.v_proj.bias', 'image_encoder.trunk.blocks.12.attn.proj.bias', 'memory_attention.layers.3.norm1.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.0.bias', 'image_encoder.trunk.blocks.19.mlp.layers.0.bias', 'image_encoder.trunk.blocks.36.norm2.bias', 'obj_ptr_proj.layers.0.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.2.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias', 'sam_prompt_encoder.mask_downscaling.1.bias', 'image_encoder.trunk.blocks.37.attn.qkv.bias', 'image_encoder.trunk.blocks.11.norm1.bias', 'memory_encoder.mask_downsampler.encoder.9.bias', 'image_encoder.trunk.blocks.31.norm1.bias', 'image_encoder.trunk.blocks.24.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.q_proj.bias', 'memory_attention.layers.2.linear1.bias', 'image_encoder.trunk.blocks.9.mlp.layers.1.bias', 'image_encoder.trunk.blocks.39.attn.qkv.bias', 'image_encoder.trunk.blocks.10.mlp.layers.1.bias', 'memory_attention.layers.2.self_attn.k_proj.bias', 'memory_attention.layers.0.cross_attn_image.out_proj.bias', 'image_encoder.neck.convs.0.conv.bias', 'image_encoder.trunk.blocks.12.mlp.layers.1.bias', 'obj_ptr_proj.layers.1.bias', 'image_encoder.trunk.blocks.3.norm1.bias', 'image_encoder.trunk.blocks.20.mlp.layers.1.bias', 'image_encoder.trunk.blocks.14.mlp.layers.1.bias', 'image_encoder.trunk.blocks.18.norm1.bias', 'memory_attention.layers.1.self_attn.q_proj.bias', 'image_encoder.trunk.blocks.47.mlp.layers.0.bias', 'image_encoder.trunk.blocks.26.attn.qkv.bias', 'image_encoder.trunk.blocks.40.attn.qkv.bias', 'image_encoder.trunk.blocks.25.norm2.bias', 'memory_attention.layers.3.cross_attn_image.v_proj.bias', 'memory_encoder.out_proj.bias', 'image_encoder.trunk.blocks.41.mlp.layers.0.bias', 'sam_mask_decoder.transformer.norm_final_attn.bias', 'image_encoder.trunk.blocks.33.norm1.bias', 'image_encoder.trunk.blocks.27.norm1.bias', 'image_encoder.trunk.blocks.43.norm1.bias', 'image_encoder.trunk.blocks.18.mlp.layers.1.bias', 'image_encoder.trunk.blocks.33.attn.proj.bias', 'image_encoder.trunk.blocks.28.norm2.bias', 'image_encoder.trunk.blocks.6.attn.qkv.bias', 'image_encoder.trunk.blocks.34.mlp.layers.1.bias', 'image_encoder.trunk.blocks.45.mlp.layers.1.bias', 'image_encoder.trunk.blocks.7.mlp.layers.0.bias', 'image_encoder.trunk.blocks.8.attn.qkv.bias', 'memory_attention.layers.1.norm3.bias', 'image_encoder.trunk.blocks.9.norm1.bias', 'image_encoder.trunk.blocks.16.mlp.layers.1.bias', 'image_encoder.trunk.blocks.4.attn.proj.bias', 'image_encoder.trunk.blocks.37.attn.proj.bias', 'image_encoder.trunk.blocks.15.norm1.bias', 'image_encoder.neck.convs.1.conv.bias', 'image_encoder.trunk.blocks.13.mlp.layers.1.bias', 'image_encoder.trunk.blocks.5.norm2.bias', 'image_encoder.trunk.blocks.38.norm1.bias', 'memory_attention.layers.1.self_attn.k_proj.bias', 'image_encoder.trunk.blocks.3.mlp.layers.0.bias', 'memory_attention.layers.3.self_attn.k_proj.bias', 'image_encoder.trunk.blocks.30.norm2.bias', 'sam_mask_decoder.transformer.layers.0.mlp.layers.0.bias', 'memory_attention.norm.bias', 'image_encoder.trunk.blocks.31.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias', 'image_encoder.trunk.blocks.7.attn.proj.bias', 'sam_mask_decoder.transformer.layers.1.mlp.layers.0.bias', 'memory_attention.layers.0.cross_attn_image.k_proj.bias', 'memory_attention.layers.0.cross_attn_image.v_proj.bias', 'image_encoder.trunk.blocks.13.norm2.bias', 'image_encoder.trunk.blocks.19.attn.proj.bias', 'memory_attention.layers.3.cross_attn_image.k_proj.bias', 'image_encoder.trunk.blocks.10.attn.qkv.bias', 'memory_encoder.mask_downsampler.encoder.0.bias', 'image_encoder.trunk.blocks.33.mlp.layers.1.bias', 'image_encoder.trunk.blocks.36.mlp.layers.0.bias', 'image_encoder.trunk.blocks.33.norm2.bias', 'image_encoder.trunk.blocks.20.norm2.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias', 'image_encoder.trunk.blocks.21.attn.qkv.bias', 'image_encoder.trunk.blocks.6.attn.proj.bias', 'image_encoder.trunk.blocks.14.norm1.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.v_proj.bias', 'image_encoder.trunk.blocks.18.attn.proj.bias', 'image_encoder.trunk.blocks.29.norm2.bias', 'image_encoder.trunk.blocks.3.attn.qkv.bias', 'memory_encoder.mask_downsampler.encoder.4.bias', 'image_encoder.trunk.blocks.26.norm2.bias', 'image_encoder.trunk.blocks.24.norm1.bias', 'image_encoder.trunk.blocks.25.mlp.layers.1.bias', 'image_encoder.trunk.blocks.40.mlp.layers.0.bias', 'memory_encoder.pix_feat_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.1.bias', 'image_encoder.trunk.blocks.4.norm2.bias', 'sam_mask_decoder.transformer.layers.1.mlp.layers.1.bias', 'image_encoder.trunk.blocks.29.attn.proj.bias', 'image_encoder.trunk.blocks.2.attn.proj.bias', 'image_encoder.trunk.blocks.15.norm2.bias'}\n",
      "INFO 2025-03-26 11:47:08,819 optimizer.py: 220: Matches for module_cls_name [torch.nn.LayerNorm]: {'image_encoder.trunk.blocks.20.norm2.weight', 'image_encoder.trunk.blocks.29.norm1.bias', 'image_encoder.trunk.blocks.30.norm1.bias', 'image_encoder.trunk.blocks.37.norm1.bias', 'image_encoder.trunk.blocks.23.norm1.weight', 'image_encoder.trunk.blocks.36.norm1.weight', 'image_encoder.trunk.blocks.42.norm2.bias', 'image_encoder.trunk.blocks.16.norm2.bias', 'image_encoder.trunk.blocks.17.norm1.weight', 'image_encoder.trunk.blocks.44.norm2.bias', 'memory_attention.layers.1.norm1.weight', 'sam_mask_decoder.transformer.layers.0.norm1.bias', 'image_encoder.trunk.blocks.4.norm1.bias', 'image_encoder.trunk.blocks.32.norm1.bias', 'memory_attention.layers.2.norm3.bias', 'image_encoder.trunk.blocks.22.norm2.bias', 'memory_attention.layers.3.norm2.bias', 'memory_attention.layers.1.norm2.weight', 'image_encoder.trunk.blocks.47.norm2.weight', 'sam_mask_decoder.transformer.layers.1.norm1.weight', 'image_encoder.trunk.blocks.21.norm2.weight', 'image_encoder.trunk.blocks.10.norm2.bias', 'image_encoder.trunk.blocks.40.norm2.weight', 'image_encoder.trunk.blocks.25.norm1.bias', 'memory_attention.layers.3.norm3.weight', 'sam_mask_decoder.transformer.layers.1.norm2.bias', 'sam_mask_decoder.transformer.layers.0.norm2.weight', 'memory_attention.layers.0.norm1.bias', 'image_encoder.trunk.blocks.40.norm2.bias', 'memory_attention.layers.0.norm2.weight', 'image_encoder.trunk.blocks.20.norm1.bias', 'image_encoder.trunk.blocks.0.norm1.bias', 'image_encoder.trunk.blocks.38.norm2.weight', 'image_encoder.trunk.blocks.7.norm2.weight', 'image_encoder.trunk.blocks.38.norm2.bias', 'memory_attention.layers.3.norm1.bias', 'image_encoder.trunk.blocks.2.norm1.weight', 'image_encoder.trunk.blocks.36.norm2.bias', 'image_encoder.trunk.blocks.13.norm1.weight', 'image_encoder.trunk.blocks.41.norm1.weight', 'image_encoder.trunk.blocks.5.norm1.bias', 'image_encoder.trunk.blocks.18.norm2.bias', 'image_encoder.trunk.blocks.31.norm2.weight', 'sam_mask_decoder.transformer.layers.1.norm4.bias', 'image_encoder.trunk.blocks.43.norm1.weight', 'image_encoder.trunk.blocks.2.norm2.bias', 'image_encoder.trunk.blocks.4.norm1.weight', 'image_encoder.trunk.blocks.7.norm2.bias', 'image_encoder.trunk.blocks.26.norm2.weight', 'image_encoder.trunk.blocks.39.norm1.weight', 'image_encoder.trunk.blocks.45.norm2.weight', 'image_encoder.trunk.blocks.17.norm1.bias', 'image_encoder.trunk.blocks.3.norm1.weight', 'image_encoder.trunk.blocks.11.norm1.bias', 'image_encoder.trunk.blocks.34.norm1.bias', 'memory_attention.layers.3.norm2.weight', 'image_encoder.trunk.blocks.24.norm2.weight', 'image_encoder.trunk.blocks.31.norm1.bias', 'image_encoder.trunk.blocks.31.norm2.bias', 'sam_mask_decoder.transformer.layers.0.norm3.bias', 'image_encoder.trunk.blocks.46.norm2.weight', 'sam_mask_decoder.transformer.layers.0.norm4.bias', 'sam_mask_decoder.transformer.layers.0.norm1.weight', 'image_encoder.trunk.blocks.16.norm2.weight', 'memory_attention.layers.2.norm1.weight', 'image_encoder.trunk.blocks.30.norm2.weight', 'image_encoder.trunk.blocks.21.norm1.bias', 'image_encoder.trunk.blocks.1.norm2.weight', 'image_encoder.trunk.blocks.11.norm2.bias', 'image_encoder.trunk.blocks.25.norm1.weight', 'image_encoder.trunk.blocks.29.norm1.weight', 'image_encoder.trunk.blocks.2.norm2.weight', 'image_encoder.trunk.blocks.5.norm1.weight', 'sam_mask_decoder.transformer.layers.1.norm3.bias', 'image_encoder.trunk.blocks.32.norm1.weight', 'image_encoder.trunk.blocks.0.norm1.weight', 'image_encoder.trunk.blocks.3.norm1.bias', 'image_encoder.trunk.blocks.37.norm2.bias', 'image_encoder.trunk.blocks.19.norm2.bias', 'image_encoder.trunk.blocks.17.norm2.bias', 'image_encoder.trunk.blocks.36.norm1.bias', 'image_encoder.trunk.blocks.42.norm1.weight', 'image_encoder.trunk.blocks.18.norm1.bias', 'image_encoder.trunk.blocks.22.norm2.weight', 'image_encoder.trunk.blocks.1.norm1.weight', 'image_encoder.trunk.blocks.44.norm1.weight', 'image_encoder.trunk.blocks.8.norm2.weight', 'image_encoder.trunk.blocks.21.norm2.bias', 'image_encoder.trunk.blocks.25.norm2.bias', 'sam_mask_decoder.transformer.layers.1.norm3.weight', 'image_encoder.trunk.blocks.22.norm1.weight', 'image_encoder.trunk.blocks.17.norm2.weight', 'image_encoder.trunk.blocks.45.norm1.bias', 'image_encoder.trunk.blocks.1.norm2.bias', 'image_encoder.trunk.blocks.8.norm1.bias', 'memory_attention.layers.0.norm2.bias', 'sam_mask_decoder.transformer.norm_final_attn.bias', 'image_encoder.trunk.blocks.47.norm1.weight', 'memory_attention.layers.2.norm3.weight', 'image_encoder.trunk.blocks.39.norm2.bias', 'image_encoder.trunk.blocks.30.norm1.weight', 'image_encoder.trunk.blocks.37.norm1.weight', 'image_encoder.trunk.blocks.33.norm1.bias', 'sam_mask_decoder.transformer.layers.0.norm2.bias', 'image_encoder.trunk.blocks.27.norm1.bias', 'image_encoder.trunk.blocks.43.norm1.bias', 'image_encoder.trunk.blocks.22.norm1.bias', 'image_encoder.trunk.blocks.47.norm1.bias', 'image_encoder.trunk.blocks.10.norm1.bias', 'image_encoder.trunk.blocks.28.norm2.bias', 'memory_attention.layers.2.norm2.bias', 'image_encoder.trunk.blocks.14.norm2.bias', 'image_encoder.trunk.blocks.19.norm1.weight', 'memory_attention.layers.1.norm3.bias', 'image_encoder.trunk.blocks.9.norm1.bias', 'memory_attention.layers.1.norm2.bias', 'image_encoder.trunk.blocks.28.norm2.weight', 'image_encoder.trunk.blocks.6.norm2.bias', 'image_encoder.trunk.blocks.27.norm2.weight', 'image_encoder.trunk.blocks.15.norm2.weight', 'image_encoder.trunk.blocks.23.norm1.bias', 'image_encoder.trunk.blocks.33.norm2.weight', 'sam_mask_decoder.transformer.layers.0.norm3.weight', 'image_encoder.trunk.blocks.7.norm1.bias', 'image_encoder.trunk.blocks.35.norm1.weight', 'image_encoder.trunk.blocks.35.norm1.bias', 'image_encoder.trunk.blocks.29.norm2.weight', 'image_encoder.trunk.blocks.4.norm2.weight', 'image_encoder.trunk.blocks.8.norm1.weight', 'image_encoder.trunk.blocks.15.norm1.bias', 'sam_mask_decoder.transformer.layers.1.norm4.weight', 'image_encoder.trunk.blocks.27.norm2.bias', 'image_encoder.trunk.blocks.25.norm2.weight', 'image_encoder.trunk.blocks.44.norm1.bias', 'image_encoder.trunk.blocks.5.norm2.bias', 'image_encoder.trunk.blocks.38.norm1.bias', 'image_encoder.trunk.blocks.9.norm2.bias', 'image_encoder.trunk.blocks.18.norm2.weight', 'image_encoder.trunk.blocks.6.norm2.weight', 'image_encoder.trunk.blocks.41.norm2.weight', 'image_encoder.trunk.blocks.3.norm2.weight', 'memory_attention.layers.0.norm3.weight', 'image_encoder.trunk.blocks.40.norm1.bias', 'memory_attention.layers.3.norm1.weight', 'image_encoder.trunk.blocks.3.norm2.bias', 'image_encoder.trunk.blocks.16.norm1.bias', 'memory_attention.layers.2.norm1.bias', 'image_encoder.trunk.blocks.26.norm1.bias', 'image_encoder.trunk.blocks.30.norm2.bias', 'image_encoder.trunk.blocks.34.norm2.bias', 'image_encoder.trunk.blocks.39.norm1.bias', 'memory_attention.norm.weight', 'image_encoder.trunk.blocks.41.norm1.bias', 'memory_attention.norm.bias', 'image_encoder.trunk.blocks.12.norm1.weight', 'image_encoder.trunk.blocks.8.norm2.bias', 'image_encoder.trunk.blocks.24.norm1.weight', 'image_encoder.trunk.blocks.41.norm2.bias', 'image_encoder.trunk.blocks.0.norm2.weight', 'image_encoder.trunk.blocks.28.norm1.weight', 'image_encoder.trunk.blocks.32.norm2.bias', 'image_encoder.trunk.blocks.12.norm1.bias', 'image_encoder.trunk.blocks.33.norm1.weight', 'image_encoder.trunk.blocks.11.norm2.weight', 'image_encoder.trunk.blocks.44.norm2.weight', 'image_encoder.trunk.blocks.46.norm2.bias', 'image_encoder.trunk.blocks.39.norm2.weight', 'image_encoder.trunk.blocks.13.norm2.bias', 'image_encoder.trunk.blocks.12.norm2.bias', 'image_encoder.trunk.blocks.2.norm1.bias', 'sam_mask_decoder.transformer.layers.0.norm4.weight', 'image_encoder.trunk.blocks.38.norm1.weight', 'image_encoder.trunk.blocks.28.norm1.bias', 'sam_mask_decoder.transformer.norm_final_attn.weight', 'memory_attention.layers.1.norm1.bias', 'memory_attention.layers.0.norm3.bias', 'image_encoder.trunk.blocks.42.norm2.weight', 'image_encoder.trunk.blocks.37.norm2.weight', 'image_encoder.trunk.blocks.35.norm2.weight', 'image_encoder.trunk.blocks.42.norm1.bias', 'image_encoder.trunk.blocks.11.norm1.weight', 'image_encoder.trunk.blocks.14.norm1.weight', 'image_encoder.trunk.blocks.34.norm1.weight', 'image_encoder.trunk.blocks.36.norm2.weight', 'image_encoder.trunk.blocks.31.norm1.weight', 'image_encoder.trunk.blocks.46.norm1.weight', 'image_encoder.trunk.blocks.12.norm2.weight', 'image_encoder.trunk.blocks.1.norm1.bias', 'image_encoder.trunk.blocks.20.norm2.bias', 'image_encoder.trunk.blocks.33.norm2.bias', 'image_encoder.trunk.blocks.9.norm1.weight', 'image_encoder.trunk.blocks.35.norm2.bias', 'image_encoder.trunk.blocks.40.norm1.weight', 'image_encoder.trunk.blocks.14.norm1.bias', 'image_encoder.trunk.blocks.34.norm2.weight', 'image_encoder.trunk.blocks.6.norm1.weight', 'image_encoder.trunk.blocks.15.norm1.weight', 'image_encoder.trunk.blocks.24.norm2.bias', 'image_encoder.trunk.blocks.27.norm1.weight', 'image_encoder.trunk.blocks.29.norm2.bias', 'memory_attention.layers.0.norm1.weight', 'image_encoder.trunk.blocks.45.norm2.bias', 'image_encoder.trunk.blocks.10.norm2.weight', 'image_encoder.trunk.blocks.10.norm1.weight', 'image_encoder.trunk.blocks.19.norm2.weight', 'image_encoder.trunk.blocks.0.norm2.bias', 'image_encoder.trunk.blocks.13.norm2.weight', 'image_encoder.trunk.blocks.47.norm2.bias', 'image_encoder.trunk.blocks.14.norm2.weight', 'image_encoder.trunk.blocks.7.norm1.weight', 'image_encoder.trunk.blocks.5.norm2.weight', 'image_encoder.trunk.blocks.26.norm1.weight', 'image_encoder.trunk.blocks.26.norm2.bias', 'image_encoder.trunk.blocks.24.norm1.bias', 'image_encoder.trunk.blocks.46.norm1.bias', 'memory_attention.layers.3.norm3.bias', 'image_encoder.trunk.blocks.32.norm2.weight', 'image_encoder.trunk.blocks.9.norm2.weight', 'image_encoder.trunk.blocks.20.norm1.weight', 'image_encoder.trunk.blocks.21.norm1.weight', 'image_encoder.trunk.blocks.4.norm2.bias', 'memory_attention.layers.1.norm3.weight', 'image_encoder.trunk.blocks.43.norm2.weight', 'image_encoder.trunk.blocks.23.norm2.weight', 'image_encoder.trunk.blocks.23.norm2.bias', 'image_encoder.trunk.blocks.13.norm1.bias', 'image_encoder.trunk.blocks.16.norm1.weight', 'image_encoder.trunk.blocks.43.norm2.bias', 'memory_attention.layers.2.norm2.weight', 'image_encoder.trunk.blocks.15.norm2.bias', 'image_encoder.trunk.blocks.6.norm1.bias', 'sam_mask_decoder.transformer.layers.1.norm1.bias', 'image_encoder.trunk.blocks.18.norm1.weight', 'image_encoder.trunk.blocks.45.norm1.weight', 'image_encoder.trunk.blocks.19.norm1.bias', 'sam_mask_decoder.transformer.layers.1.norm2.weight'} \n",
      "Raw dataset length = 1342\n",
      "INFO 2025-03-26 11:47:08,965 sam2_datasets.py: 125: Dataset mixing probabilities: [1.0]\n",
      "Raw dataset length = 6110\n",
      "INFO 2025-03-26 11:47:08,981 sam2_datasets.py: 125: Dataset mixing probabilities: [1.0]\n",
      "INFO 2025-03-26 11:47:09,470 trainer.py: 417: Loading pretrained checkpoint from {'_partial_': True, '_target_': 'training.utils.checkpoint_utils.load_state_dict_into_model', 'strict': False, 'ignore_unexpected_keys': None, 'ignore_missing_keys': None, 'state_dict': {'_target_': 'training.utils.checkpoint_utils.load_checkpoint_and_apply_kernels', 'checkpoint_path': './checkpoints/sam2.1_hiera_large.pt', 'ckpt_state_dict_keys': ['model']}}\n",
      "WARNING 2025-03-26 11:47:09,534 checkpoint_utils.py: 325: State key mismatch. Missing keys: ['image_encoder.trunk.blocks.0.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.0.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.0.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.0.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.1.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.1.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.1.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.1.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.2.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.2.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.2.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.2.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.3.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.3.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.3.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.3.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.4.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.4.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.4.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.4.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.5.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.5.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.5.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.5.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.6.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.6.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.6.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.6.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.7.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.7.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.7.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.7.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.8.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.8.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.8.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.8.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.9.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.9.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.9.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.9.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.10.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.10.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.10.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.10.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.11.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.11.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.11.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.11.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.12.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.12.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.12.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.12.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.13.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.13.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.13.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.13.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.14.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.14.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.14.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.14.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.15.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.15.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.15.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.15.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.16.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.16.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.16.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.16.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.17.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.17.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.17.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.17.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.18.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.18.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.18.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.18.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.19.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.19.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.19.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.19.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.20.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.20.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.20.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.20.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.21.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.21.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.21.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.21.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.22.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.22.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.22.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.22.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.23.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.23.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.23.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.23.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.24.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.24.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.24.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.24.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.25.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.25.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.25.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.25.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.26.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.26.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.26.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.26.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.27.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.27.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.27.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.27.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.28.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.28.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.28.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.28.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.29.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.29.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.29.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.29.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.30.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.30.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.30.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.30.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.31.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.31.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.31.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.31.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.32.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.32.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.32.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.32.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.33.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.33.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.33.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.33.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.34.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.34.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.34.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.34.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.35.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.35.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.35.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.35.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.36.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.36.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.36.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.36.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.37.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.37.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.37.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.37.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.38.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.38.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.38.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.38.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.39.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.39.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.39.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.39.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.40.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.40.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.40.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.40.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.41.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.41.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.41.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.41.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.42.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.42.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.42.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.42.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.43.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.43.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.43.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.43.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.44.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.44.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.44.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.44.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.45.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.45.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.45.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.45.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.46.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.46.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.46.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.46.attn.lora_v.linear2.weight', 'image_encoder.trunk.blocks.47.attn.lora_q.linear1.weight', 'image_encoder.trunk.blocks.47.attn.lora_q.linear2.weight', 'image_encoder.trunk.blocks.47.attn.lora_v.linear1.weight', 'image_encoder.trunk.blocks.47.attn.lora_v.linear2.weight'].\n"
     ]
    }
   ],
   "source": [
    "local_rank=0\n",
    "world_size=num_proc\n",
    "\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = str(main_port)\n",
    "os.environ[\"RANK\"] = str(local_rank)\n",
    "os.environ[\"LOCAL_RANK\"] = str(local_rank)\n",
    "os.environ[\"WORLD_SIZE\"] = str(world_size)\n",
    "\n",
    "trainer = instantiate(cfg.trainer, _recursive_=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'_target_': 'training.dataset.sam2_datasets.TorchTrainMixedDataset', 'phases_per_epoch': '${scratch.phases_per_epoch}', 'batch_sizes': ['${scratch.train_batch_size}'], 'datasets': [{'_target_': 'training.dataset.utils.RepeatFactorWrapper', 'dataset': {'_target_': 'training.dataset.utils.ConcatDataset', 'datasets': [{'_target_': 'training.dataset.vos_dataset.VOSDataset', 'transforms': '${vos.train_transforms}', 'training': True, 'video_dataset': {'_target_': 'training.dataset.vos_raw_dataset.PNGRawDataset', 'img_folder': '${dataset.train_img_folder}', 'gt_folder': '${dataset.train_gt_folder}', 'file_list_txt': '${dataset.file_list_txt}'}, 'sampler': {'_target_': 'training.dataset.vos_sampler.RandomUniformSampler', 'num_frames': '${scratch.num_frames}', 'max_num_objects': '${scratch.max_num_objects}'}, 'multiplier': '${dataset.multiplier}'}]}}], 'shuffle': True, 'num_workers': '${scratch.num_train_workers}', 'pin_memory': True, 'drop_last': True, 'collate_fn': {'_target_': 'training.utils.data_utils.collate_fn', '_partial_': True, 'dict_key': 'all'}}, 'val': {'_target_': 'training.dataset.sam2_datasets.TorchTrainMixedDataset', 'phases_per_epoch': '${scratch.phases_per_epoch}', 'batch_sizes': ['${scratch.train_batch_size}'], 'datasets': [{'_target_': 'training.dataset.utils.RepeatFactorWrapper', 'dataset': {'_target_': 'training.dataset.utils.ConcatDataset', 'datasets': [{'_target_': 'training.dataset.vos_dataset.VOSDataset', 'transforms': '${vos.val_transforms}', 'training': False, 'video_dataset': {'_target_': 'training.dataset.vos_raw_dataset.PNGRawDataset', 'img_folder': '${dataset.val_img_folder}', 'gt_folder': '${dataset.val_gt_folder}', 'file_list_txt': '${dataset.file_list_txt}'}, 'sampler': {'_target_': 'training.dataset.vos_sampler.EvalSampler'}, 'multiplier': '${dataset.multiplier}'}]}}], 'shuffle': False, 'num_workers': '${scratch.num_train_workers}', 'pin_memory': True, 'drop_last': False, 'collate_fn': {'_target_': 'training.utils.data_utils.collate_fn', '_partial_': True, 'dict_key': 'val'}}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.trainer.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samba/320281459/code/sam2/training/trainer.py:626: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-03-26 11:55:12,470 train_utils.py: 271: Val Epoch: [0][  0/135] | Batch Time: 5.40 (5.40) | Data Time: 2.77 (2.77) | Mem (GB): 3.00 (3.00/3.00) | Time Elapsed: 00d 00h 08m | Losses/val_all_loss: 0.00e+00 (0.00e+00) | Losses/val_val_loss: 7.33e+00 (7.33e+00)\n",
      "INFO 2025-03-26 11:55:28,446 train_utils.py: 271: Val Epoch: [0][ 10/135] | Batch Time: 1.43 (1.94) | Data Time: 0.00 (0.25) | Mem (GB): 3.00 (2.91/3.00) | Time Elapsed: 00d 00h 08m | Losses/val_all_loss: 0.00e+00 (0.00e+00) | Losses/val_val_loss: 7.49e+00 (7.34e+00)\n",
      "INFO 2025-03-26 11:55:43,128 train_utils.py: 271: Val Epoch: [0][ 20/135] | Batch Time: 1.45 (1.72) | Data Time: 0.00 (0.13) | Mem (GB): 3.00 (2.90/3.00) | Time Elapsed: 00d 00h 08m | Losses/val_all_loss: 0.00e+00 (0.00e+00) | Losses/val_val_loss: 6.65e+00 (7.21e+00)\n",
      "INFO 2025-03-26 11:55:58,223 train_utils.py: 271: Val Epoch: [0][ 30/135] | Batch Time: 1.56 (1.65) | Data Time: 0.00 (0.09) | Mem (GB): 3.00 (2.90/3.00) | Time Elapsed: 00d 00h 08m | Losses/val_all_loss: 0.00e+00 (0.00e+00) | Losses/val_val_loss: 6.88e+00 (7.08e+00)\n",
      "INFO 2025-03-26 11:56:11,362 train_utils.py: 271: Val Epoch: [0][ 40/135] | Batch Time: 1.36 (1.57) | Data Time: 0.00 (0.07) | Mem (GB): 3.00 (2.90/3.00) | Time Elapsed: 00d 00h 09m | Losses/val_all_loss: 0.00e+00 (0.00e+00) | Losses/val_val_loss: 5.83e+00 (6.97e+00)\n",
      "INFO 2025-03-26 11:56:24,100 train_utils.py: 271: Val Epoch: [0][ 50/135] | Batch Time: 1.32 (1.51) | Data Time: 0.00 (0.06) | Mem (GB): 3.00 (2.92/3.00) | Time Elapsed: 00d 00h 09m | Losses/val_all_loss: 0.00e+00 (0.00e+00) | Losses/val_val_loss: 5.61e+00 (6.87e+00)\n",
      "INFO 2025-03-26 11:56:36,728 train_utils.py: 271: Val Epoch: [0][ 60/135] | Batch Time: 1.30 (1.47) | Data Time: 0.00 (0.05) | Mem (GB): 3.00 (2.93/3.00) | Time Elapsed: 00d 00h 09m | Losses/val_all_loss: 0.00e+00 (0.00e+00) | Losses/val_val_loss: 7.38e+00 (6.89e+00)\n",
      "INFO 2025-03-26 11:56:49,138 train_utils.py: 271: Val Epoch: [0][ 70/135] | Batch Time: 1.21 (1.44) | Data Time: 0.00 (0.04) | Mem (GB): 3.00 (2.90/3.00) | Time Elapsed: 00d 00h 09m | Losses/val_all_loss: 0.00e+00 (0.00e+00) | Losses/val_val_loss: 5.32e+00 (6.84e+00)\n",
      "INFO 2025-03-26 11:57:02,155 train_utils.py: 271: Val Epoch: [0][ 80/135] | Batch Time: 1.30 (1.42) | Data Time: 0.00 (0.04) | Mem (GB): 3.00 (2.91/3.00) | Time Elapsed: 00d 00h 09m | Losses/val_all_loss: 0.00e+00 (0.00e+00) | Losses/val_val_loss: 6.09e+00 (6.84e+00)\n",
      "INFO 2025-03-26 11:57:14,891 train_utils.py: 271: Val Epoch: [0][ 90/135] | Batch Time: 1.38 (1.40) | Data Time: 0.00 (0.03) | Mem (GB): 3.00 (2.92/3.00) | Time Elapsed: 00d 00h 10m | Losses/val_all_loss: 0.00e+00 (0.00e+00) | Losses/val_val_loss: 5.53e+00 (6.75e+00)\n",
      "INFO 2025-03-26 11:57:27,182 train_utils.py: 271: Val Epoch: [0][100/135] | Batch Time: 1.11 (1.39) | Data Time: 0.00 (0.03) | Mem (GB): 3.00 (2.91/3.00) | Time Elapsed: 00d 00h 10m | Losses/val_all_loss: 0.00e+00 (0.00e+00) | Losses/val_val_loss: 6.14e+00 (6.76e+00)\n",
      "INFO 2025-03-26 11:57:40,570 train_utils.py: 271: Val Epoch: [0][110/135] | Batch Time: 1.50 (1.38) | Data Time: 0.00 (0.03) | Mem (GB): 3.00 (2.91/3.00) | Time Elapsed: 00d 00h 10m | Losses/val_all_loss: 0.00e+00 (0.00e+00) | Losses/val_val_loss: 5.58e+00 (6.77e+00)\n",
      "INFO 2025-03-26 11:57:56,163 train_utils.py: 271: Val Epoch: [0][120/135] | Batch Time: 1.38 (1.40) | Data Time: 0.00 (0.02) | Mem (GB): 3.00 (2.91/3.00) | Time Elapsed: 00d 00h 10m | Losses/val_all_loss: 0.00e+00 (0.00e+00) | Losses/val_val_loss: 6.50e+00 (6.77e+00)\n",
      "INFO 2025-03-26 11:58:11,409 train_utils.py: 271: Val Epoch: [0][130/135] | Batch Time: 1.35 (1.41) | Data Time: 0.00 (0.02) | Mem (GB): 2.00 (2.89/3.00) | Time Elapsed: 00d 00h 11m | Losses/val_all_loss: 0.00e+00 (0.00e+00) | Losses/val_val_loss: 6.61e+00 (6.80e+00)\n",
      "INFO 2025-03-26 11:58:17,312 trainer.py: 950: Estimated time remaining: 00d 02h 03m\n",
      "INFO 2025-03-26 11:58:17,315 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-03-26 11:58:17,316 trainer.py: 693: Meters: {'Losses/val_all_loss': 0, 'Losses/val_val_loss': 6.802880965338813, 'Losses/val_val_loss_mask': 0.08701911885981206, 'Losses/val_val_loss_dice': 3.6583795847716156, 'Losses/val_val_loss_iou': 1.4041186226738824, 'Losses/val_val_loss_class': 3.976632575165274e-07, 'Losses/val_val_core_loss': 6.802880965338813, 'Trainer/where': 0.0, 'Trainer/epoch': 0, 'Trainer/steps_val': 135}\n"
     ]
    }
   ],
   "source": [
    "trainer.run_val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80850178\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistributedDataParallel(\n",
       "  (module): SAM2Train(\n",
       "    (image_encoder): ImageEncoder(\n",
       "      (trunk): Hiera(\n",
       "        (patch_embed): PatchEmbed(\n",
       "          (proj): Conv2d(3, 112, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "        )\n",
       "        (blocks): ModuleList(\n",
       "          (0): MultiScaleBlock(\n",
       "            (norm1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): MultiScaleAttention(\n",
       "              (qkv): Linear(in_features=112, out_features=336, bias=True)\n",
       "              (proj): Linear(in_features=112, out_features=112, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((112,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): MLP(\n",
       "              (layers): ModuleList(\n",
       "                (0): Linear(in_features=112, out_features=448, bias=True)\n",
       "                (1): Linear(in_features=448, out_features=112, bias=True)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "            )\n",
       "          )\n",
       "          (1): MultiScaleBlock(\n",
       "            (norm1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): MultiScaleAttention(\n",
       "              (qkv): Linear(in_features=112, out_features=336, bias=True)\n",
       "              (proj): Linear(in_features=112, out_features=112, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((112,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): MLP(\n",
       "              (layers): ModuleList(\n",
       "                (0): Linear(in_features=112, out_features=448, bias=True)\n",
       "                (1): Linear(in_features=448, out_features=112, bias=True)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "            )\n",
       "          )\n",
       "          (2): MultiScaleBlock(\n",
       "            (norm1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)\n",
       "            (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "            (attn): MultiScaleAttention(\n",
       "              (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "              (qkv): Linear(in_features=112, out_features=672, bias=True)\n",
       "              (proj): Linear(in_features=224, out_features=224, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((224,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): MLP(\n",
       "              (layers): ModuleList(\n",
       "                (0): Linear(in_features=224, out_features=896, bias=True)\n",
       "                (1): Linear(in_features=896, out_features=224, bias=True)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "            )\n",
       "            (proj): Linear(in_features=112, out_features=224, bias=True)\n",
       "          )\n",
       "          (3-4): 2 x MultiScaleBlock(\n",
       "            (norm1): LayerNorm((224,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): MultiScaleAttention(\n",
       "              (qkv): Linear(in_features=224, out_features=672, bias=True)\n",
       "              (proj): Linear(in_features=224, out_features=224, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((224,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): MLP(\n",
       "              (layers): ModuleList(\n",
       "                (0): Linear(in_features=224, out_features=896, bias=True)\n",
       "                (1): Linear(in_features=896, out_features=224, bias=True)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "            )\n",
       "          )\n",
       "          (5): MultiScaleBlock(\n",
       "            (norm1): LayerNorm((224,), eps=1e-06, elementwise_affine=True)\n",
       "            (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "            (attn): MultiScaleAttention(\n",
       "              (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "              (qkv): Linear(in_features=224, out_features=1344, bias=True)\n",
       "              (proj): Linear(in_features=448, out_features=448, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((448,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): MLP(\n",
       "              (layers): ModuleList(\n",
       "                (0): Linear(in_features=448, out_features=1792, bias=True)\n",
       "                (1): Linear(in_features=1792, out_features=448, bias=True)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "            )\n",
       "            (proj): Linear(in_features=224, out_features=448, bias=True)\n",
       "          )\n",
       "          (6-20): 15 x MultiScaleBlock(\n",
       "            (norm1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): MultiScaleAttention(\n",
       "              (qkv): Linear(in_features=448, out_features=1344, bias=True)\n",
       "              (proj): Linear(in_features=448, out_features=448, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((448,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): MLP(\n",
       "              (layers): ModuleList(\n",
       "                (0): Linear(in_features=448, out_features=1792, bias=True)\n",
       "                (1): Linear(in_features=1792, out_features=448, bias=True)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "            )\n",
       "          )\n",
       "          (21): MultiScaleBlock(\n",
       "            (norm1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)\n",
       "            (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "            (attn): MultiScaleAttention(\n",
       "              (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "              (qkv): Linear(in_features=448, out_features=2688, bias=True)\n",
       "              (proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((896,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): MLP(\n",
       "              (layers): ModuleList(\n",
       "                (0): Linear(in_features=896, out_features=3584, bias=True)\n",
       "                (1): Linear(in_features=3584, out_features=896, bias=True)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "            )\n",
       "            (proj): Linear(in_features=448, out_features=896, bias=True)\n",
       "          )\n",
       "          (22-23): 2 x MultiScaleBlock(\n",
       "            (norm1): LayerNorm((896,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): MultiScaleAttention(\n",
       "              (qkv): Linear(in_features=896, out_features=2688, bias=True)\n",
       "              (proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((896,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): MLP(\n",
       "              (layers): ModuleList(\n",
       "                (0): Linear(in_features=896, out_features=3584, bias=True)\n",
       "                (1): Linear(in_features=3584, out_features=896, bias=True)\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (neck): FpnNeck(\n",
       "        (position_encoding): PositionEmbeddingSine()\n",
       "        (convs): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (conv): Conv2d(896, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (conv): Conv2d(448, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (conv): Conv2d(224, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (3): Sequential(\n",
       "            (conv): Conv2d(112, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mask_downsample): Conv2d(1, 1, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (memory_attention): MemoryAttention(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x MemoryAttentionLayer(\n",
       "          (self_attn): RoPEAttention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (cross_attn_image): RoPEAttention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (memory_encoder): MemoryEncoder(\n",
       "      (mask_downsampler): MaskDownSampler(\n",
       "        (encoder): Sequential(\n",
       "          (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (1): LayerNorm2d()\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (4): LayerNorm2d()\n",
       "          (5): GELU(approximate='none')\n",
       "          (6): Conv2d(16, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (7): LayerNorm2d()\n",
       "          (8): GELU(approximate='none')\n",
       "          (9): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (10): LayerNorm2d()\n",
       "          (11): GELU(approximate='none')\n",
       "          (12): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (pix_feat_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (fuser): Fuser(\n",
       "        (proj): Identity()\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x CXBlock(\n",
       "            (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
       "            (norm): LayerNorm2d()\n",
       "            (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (position_encoding): PositionEmbeddingSine()\n",
       "      (out_proj): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (sam_prompt_encoder): PromptEncoder(\n",
       "      (pe_layer): PositionEmbeddingRandom()\n",
       "      (point_embeddings): ModuleList(\n",
       "        (0-3): 4 x Embedding(1, 256)\n",
       "      )\n",
       "      (not_a_point_embed): Embedding(1, 256)\n",
       "      (mask_downscaling): Sequential(\n",
       "        (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): LayerNorm2d()\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (no_mask_embed): Embedding(1, 256)\n",
       "    )\n",
       "    (sam_mask_decoder): MaskDecoder(\n",
       "      (transformer): TwoWayTransformer(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TwoWayAttentionBlock(\n",
       "            (self_attn): Attention(\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_token_to_image): Attention(\n",
       "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLP(\n",
       "              (layers): ModuleList(\n",
       "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
       "                (1): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              )\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_image_to_token): Attention(\n",
       "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_attn_token_to_image): Attention(\n",
       "          (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (iou_token): Embedding(1, 256)\n",
       "      (mask_tokens): Embedding(4, 256)\n",
       "      (obj_score_token): Embedding(1, 256)\n",
       "      (output_upscaling): Sequential(\n",
       "        (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): GELU(approximate='none')\n",
       "      )\n",
       "      (conv_s0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv_s1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (output_hypernetworks_mlps): ModuleList(\n",
       "        (0-3): 4 x MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "          )\n",
       "          (act): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (iou_prediction_head): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "        )\n",
       "        (act): ReLU()\n",
       "      )\n",
       "      (pred_obj_score_head): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "        )\n",
       "        (act): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (obj_ptr_proj): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (obj_ptr_tpos_proj): Linear(in_features=256, out_features=64, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = trainer.model\n",
    "\n",
    "trainable_parameters = sum(\n",
    "    p.numel() for p in model.parameters(**{}) if p.requires_grad\n",
    ")\n",
    "total_parameters = sum(p.numel() for p in model.parameters(**{}))\n",
    "non_trainable_parameters = total_parameters - trainable_parameters\n",
    "print(trainable_parameters)\n",
    "print(non_trainable_parameters)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
